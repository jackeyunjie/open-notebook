"""P0 Layer Agents - Perception Layer for Organic Growth System.

This module provides the foundationalÊÑüÁü•Â±Ç (Perception Layer) agents that form
the "sensory system" of the organic growth architecture.

Architecture:
    P0 Layer (Perception) ‚Üê You are here
    P1 Layer (Value Judgment)
    P2 Layer (Relationship)
    P3 Layer (Evolution)

Four Quadrant Agents at P0:
    - Q1P0 PainScannerAgent: Scans for user painpoints (instant, continuous, hidden)
    - Q2P0 EmotionWatcherAgent: Monitors emotional signals
    - Q3P0 TrendHunterAgent: Tracks trending topics
    - Q4P0 SceneDiscoverAgent: Discovers potential usage scenarios

Daily Sync Protocol:
    All P0 agents participate in daily synchronization to share signals
    and enable cross-quadrant intelligence.
"""

import asyncio
import json
import re
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Set

from loguru import logger

from open_notebook.domain.notebook import Note, Source
from open_notebook.skills.base import Skill, SkillConfig, SkillContext, SkillResult, SkillStatus
from open_notebook.skills.registry import register_skill


class PainPointType(str, Enum):
    """Three types of painpoints per VIKKI methodology."""
    INSTANT = "instant"       # Âç≥Êó∂ÊÄßÁóõÁÇπ - Urgent, time-sensitive
    CONTINUOUS = "continuous" # ÊåÅÁª≠ÊÄßÁóõÁÇπ - Ongoing, persistent
    HIDDEN = "hidden"         # ÈöêÊÄßÁóõÁÇπ - Unspoken, underlying


class PainSignal:
    """A detected pain signal with metadata."""

    def __init__(
        self,
        text: str,
        pain_type: PainPointType,
        urgency_score: int,  # 0-100
        source_platform: str,  # xiaohongshu, douyin, baidu, etc.
        detected_at: datetime = None,
        keywords: List[str] = None,
        user_scenario: str = "",
        trend_direction: str = "stable",  # rising, falling, stable
        volume_estimate: Optional[int] = None,
        related_questions: List[str] = None
    ):
        self.text = text
        self.pain_type = pain_type
        self.urgency_score = urgency_score
        self.source_platform = source_platform
        self.detected_at = detected_at or datetime.utcnow()
        self.keywords = keywords or []
        self.user_scenario = user_scenario
        self.trend_direction = trend_direction
        self.volume_estimate = volume_estimate
        self.related_questions = related_questions or []

    def to_dict(self) -> Dict[str, Any]:
        return {
            "text": self.text,
            "pain_type": self.pain_type.value,
            "urgency_score": self.urgency_score,
            "source_platform": self.source_platform,
            "detected_at": self.detected_at.isoformat(),
            "keywords": self.keywords,
            "user_scenario": self.user_scenario,
            "trend_direction": self.trend_direction,
            "volume_estimate": self.volume_estimate,
            "related_questions": self.related_questions
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "PainSignal":
        return cls(
            text=data["text"],
            pain_type=PainPointType(data["pain_type"]),
            urgency_score=data["urgency_score"],
            source_platform=data["source_platform"],
            detected_at=datetime.fromisoformat(data["detected_at"]),
            keywords=data.get("keywords", []),
            user_scenario=data.get("user_scenario", ""),
            trend_direction=data.get("trend_direction", "stable"),
            volume_estimate=data.get("volume_estimate"),
            related_questions=data.get("related_questions", [])
        )


class DailySyncReport:
    """Report generated by P0 agents during daily sync."""

    def __init__(self, agent_name: str, quadrant: str, layer: str = "P0"):
        self.agent_name = agent_name
        self.quadrant = quadrant
        self.layer = layer
        self.generated_at = datetime.utcnow()
        self.signals: List[Dict[str, Any]] = []
        self.insights: List[str] = []
        self.requests_to_other_quadrants: List[Dict[str, Any]] = []

    def add_signal(self, signal: Dict[str, Any], priority: str = "normal"):
        """Add a signal with priority (high/normal/low)."""
        self.signals.append({
            "signal": signal,
            "priority": priority,
            "timestamp": datetime.utcnow().isoformat()
        })

    def add_insight(self, insight: str):
        """Add a cross-quadrant insight."""
        self.insights.append(insight)

    def request_collaboration(self, target_quadrant: str, request_type: str, payload: Dict):
        """Request collaboration from another quadrant agent."""
        self.requests_to_other_quadrants.append({
            "target_quadrant": target_quadrant,
            "request_type": request_type,
            "payload": payload,
            "timestamp": datetime.utcnow().isoformat()
        })

    def to_dict(self) -> Dict[str, Any]:
        return {
            "agent_name": self.agent_name,
            "quadrant": self.quadrant,
            "layer": self.layer,
            "generated_at": self.generated_at.isoformat(),
            "signals_count": len(self.signals),
            "signals": self.signals,
            "insights": self.insights,
            "requests_to_other_quadrants": self.requests_to_other_quadrants
        }


@register_skill
class PainScannerAgent(Skill):
    """Q1P0 Agent: Continuously scans for user painpoints across platforms.

    This is the P0 (Perception Layer) agent for Quadrant 1 (Intent Users).
    It forms the "pain sensory system" of the organic growth architecture.

    Capabilities:
        1. Multi-platform painpoint scanning (Xiaohongshu, Douyin, Baidu, etc.)
        2. Three-type painpoint detection (instant, continuous, hidden)
        3. Urgency scoring and trend analysis
        4. Daily sync with other P0 agents
        5. Signal persistence to SharedMemory

    Data Sources:
        - Search platforms: Baidu, WeChat Search, Xiaohongshu, Douyin
        - User interactions: Comments, private messages, community discussions
        - Competitor content: Trending content in the same niche

    Output Format:
        PainSignal objects containing painpoint text, type, urgency score,
        source platform, and trend metadata.

    Integration:
        - Upwards: Feeds data to Q1P1 (Value Judgment) for prioritization
        - Sideways: Daily sync with Q2P0, Q3P0, Q4P0 for cross-quadrant intelligence
        - Downwards: Stores signals in SharedMemory for upper layers to access

    Example:
        config = SkillConfig(
            skill_type="pain_scanner_agent",
            name="Pain Scanner Agent Q1P0",
            parameters={
                "platforms": ["xiaohongshu", "douyin"],
                "pain_types": ["instant", "continuous", "hidden"],
                "scan_interval_hours": 4,
                "daily_sync_enabled": True
            }
        )
    """

    skill_type = "pain_scanner_agent"
    name = "Pain Scanner Agent (Q1P0)"
    description = "P0 perception agent that continuously scans for user painpoints"

    # Urgency keywords for instant painpoints (‰∏≠Ëã±ÂèåËØ≠)
    INSTANT_KEYWORDS = [
        # ‰∏≠Êñá
        "Êù•‰∏çÂèä‰∫Ü", "È©¨‰∏ä", "Á´ãÂàª", "Áé∞Âú®", "‰ªäÊôö", "ÊòéÂ§©", "Á¥ßÊÄ•", "ÊÄ•",
        "ÊÄé‰πàÂäû", "ÊïëÂëΩ", "Ê±ÇÂä©", "Âú®Á∫øÁ≠â", "Êå∫ÊÄ•ÁöÑ", "È©¨‰∏äË¶ÅÁî®",
        "È©¨‰∏äÂ∞±Ë¶Å", "Á´ãÂàªÈúÄË¶Å", "‰ªäÊôöÂøÖÈ°ª", "ÊòéÂ§©Â∞±Ë¶Å", "ÊÄ•Ê±Ç",
        # English
        "urgent", "emergency", "now", "immediately", "asap", "today", "tonight",
        "tomorrow", "right now", "quickly", "hurry", "deadline", "due",
        "help", "how to", "what should", "need now", "running out"
    ]

    # Keywords for continuous painpoints
    CONTINUOUS_KEYWORDS = [
        # ‰∏≠Êñá
        "ÊÄªÊòØ", "‰∏ÄÁõ¥", "ÊØèÂ§©", "ÈïøÊúü", "ÊåÅÁª≠", "ÂèçÂ§ç", "ËÄÅÊòØ",
        "ÊÄªÊòØËøôÊ†∑", "‰∏ÄÁõ¥Êó†Ê≥ï", "Âèà", "ÂèàÊòØ", "ÂÜçÊ¨°", "ËøòÊòØ",
        "Âõ∞Êâ∞", "ÊäòÁ£®", "Âèó‰∏ç‰∫Ü", "‰π†ÊÉØ‰∫Ü‰ΩÜËøòÊòØ", "ÊØèÂπ¥ÈÉΩ",
        "ÊØè‰∏™ÊúàÈÉΩ", "ÊØèÂë®ÈÉΩ", "ÊØèÊ¨°", "‰∏ÄËÄåÂÜç",
        # English
        "always", "constantly", "every day", "every night", "all the time",
        "keep having", "never stops", "ongoing", "chronic", "persistent",
        "again and again", "every time", "still", "yet", "can\'t seem"
    ]

    # Indicators for hidden painpoints
    HIDDEN_INDICATORS = [
        # ‰∏≠Êñá
        "Â∏åÊúõËÉΩ", "Ë¶ÅÊòØ", " secretly", "‰∏çÂ•ΩÊÑèÊÄùËØ¥", "‰∏çÊï¢ÊâøËÆ§",
        "ÂÖ∂ÂÆûÊÉ≥", "ÂøÉÈáåÊÉ≥Ë¶Å", "‰∏çÊï¢ËÆ©Âà´‰∫∫Áü•ÈÅì", "ÊÄïË¢´ËØ¥",
        "ÊÄïË¢´ÂèëÁé∞", "ÂÅ∑ÂÅ∑", "ÊöóÂú∞Èáå", "Ë°®Èù¢‰∏ä", "ÂÜÖÂøÉ",
        "ÁúüÊ≠£ÊÉ≥Ë¶Å", "ÂÖ∂ÂÆû", "ËØ¥ÂÆûËØù", "ËØ¥Âè•ÂøÉÈáåËØù",
        # English
        "wish i could", "if only", "secretly", "nobody knows",
        "embarrassed to say", "afraid to admit", "don\'t tell",
        "actually want", "deep down", "in my heart", "pretend"
    ]

    parameters_schema = {
        "platforms": {
            "type": "array",
            "items": {"type": "string"},
            "default": ["xiaohongshu", "douyin", "baidu"],
            "description": "Platforms to scan for painpoints"
        },
        "pain_types": {
            "type": "array",
            "items": {"type": "string", "enum": ["instant", "continuous", "hidden"]},
            "default": ["instant", "continuous", "hidden"],
            "description": "Which painpoint types to detect"
        },
        "scan_interval_hours": {
            "type": "integer",
            "default": 4,
            "minimum": 1,
            "maximum": 24,
            "description": "Hours between automatic scans"
        },
        "daily_sync_enabled": {
            "type": "boolean",
            "default": True,
            "description": "Enable daily sync with other P0 agents"
        },
        "min_urgency_score": {
            "type": "integer",
            "default": 60,
            "minimum": 0,
            "maximum": 100,
            "description": "Minimum urgency threshold for reporting"
        },
        "max_signals_per_scan": {
            "type": "integer",
            "default": 20,
            "minimum": 5,
            "maximum": 100,
            "description": "Maximum signals to collect per scan"
        },
        "target_keywords": {
            "type": "array",
            "items": {"type": "string"},
            "default": [],
            "description": "Specific keywords to monitor (empty = auto-detect)"
        },
        "source_ids": {
            "type": "array",
            "items": {"type": "string"},
            "default": [],
            "description": "Source IDs to analyze (local content)"
        },
        "text_content": {
            "type": "string",
            "default": "",
            "description": "Direct text content to analyze"
        }
    }

    def __init__(self, config: SkillConfig):
        self.platforms: List[str] = config.parameters.get("platforms", ["xiaohongshu", "douyin"])
        self.pain_types: List[str] = config.parameters.get("pain_types", ["instant", "continuous", "hidden"])
        self.scan_interval_hours: int = config.parameters.get("scan_interval_hours", 4)
        self.daily_sync_enabled: bool = config.parameters.get("daily_sync_enabled", True)
        self.min_urgency_score: int = config.parameters.get("min_urgency_score", 60)
        self.max_signals_per_scan: int = config.parameters.get("max_signals_per_scan", 20)
        self.target_keywords: List[str] = config.parameters.get("target_keywords", [])
        self.source_ids: List[str] = config.parameters.get("source_ids", [])
        self.text_content: str = config.parameters.get("text_content", "")

        # Signal storage (in production, this would be a database)
        self.detected_signals: List[PainSignal] = []
        self.last_scan_time: Optional[datetime] = None

        super().__init__(config)

    def _validate_config(self) -> None:
        super()._validate_config()
        valid_platforms = ["xiaohongshu", "douyin", "baidu", "wechat", "zhihu"]
        for platform in self.platforms:
            if platform not in valid_platforms:
                logger.warning(f"Platform {platform} may not be fully supported yet")

    async def _fetch_platform_data(self, platform: str) -> str:
        """Fetch raw data from external platform (simulated for now).

        In production, this would:
        - Call platform APIs (Xiaohongshu, Douyin, etc.)
        - Use web scraping where APIs unavailable
        - Handle rate limiting and authentication
        """
        logger.info(f"Fetching data from {platform}...")

        # TODO: Implement real platform integrations
        # For now, return placeholder data or use provided content
        if self.text_content:
            return self.text_content

        # Simulate platform data
        simulated_data = {
            "xiaohongshu": self._simulate_xiaohongshu_data(),
            "douyin": self._simulate_douyin_data(),
            "baidu": self._simulate_baidu_data(),
            "wechat": self._simulate_wechat_data(),
            "zhihu": self._simulate_zhihu_data()
        }

        return simulated_data.get(platform, "")

    def _simulate_xiaohongshu_data(self) -> str:
        """Simulate Xiaohongshu search/trend data."""
        # In production: call Xiaohongshu API or scrape
        return """
        ÊâãÊú∫ÂÜÖÂ≠ò‰∏çË∂≥ÊÄé‰πàÂäû È©¨‰∏äÂ∞±Ë¶ÅÊª°‰∫Ü
        iPhoneÂ≠òÂÇ®Á©∫Èó¥‰∏çË∂≥ÊÄé‰πàÊ∏ÖÁêÜ ÊÄ•
        ÊòéÂ§©HRÁ∫¶Ë∞àÁ¶ªËÅåËµîÂÅøÊÄé‰πàÁÆó Âú®Á∫øÁ≠â
        ÊÄªÊòØÊó©Ëµ∑Â§±Ë¥• ÊØèÂ§©ÈóπÈíüÂìç‰∫ÜËøòÊòØËµ∑‰∏çÊù•
        Ë£Ö‰øÆÈ¢ÑÁÆóÊÄªÊòØË∂Ö ÊØè‰∏™ÊúàÈÉΩË∂ÖÊîØ
        Â∏åÊúõËÉΩÁî®AI‰ΩÜÊÄïË¢´ÂèëÁé∞ ‰∏çÊï¢ËÆ©Âà´‰∫∫Áü•ÈÅì
        """

    def _simulate_douyin_data(self) -> str:
        """Simulate Douyin trend data."""
        return """
        ‰ªäÊôöÊºîÂî±‰ºöÊä¢Á•®ÊîªÁï• Êù•‰∏çÂèä‰∫Ü
        ÊòéÂ§©ËÄÉËØïÊÄé‰πàÁ™ÅÂáª Á¥ßÊÄ•Ê±ÇÂä©
        ÊÄªÊòØÂùöÊåÅ‰∏ç‰∫ÜÂÅ•Ë∫´ ÊØèÊ¨°ÂäûÂç°ÈÉΩÊòØÂâçÂá†Â§©Âéª
        ÂÆùÂÆùËæÖÈ£üÊÄé‰πàÂÅö Â©ÜÂ©ÜËØ¥ÊàëÂÅ∑Êáí
        """

    def _simulate_baidu_data(self) -> str:
        """Simulate Baidu search trend data."""
        return """
        ÊâãÊú∫ÂÜÖÂ≠ò‰∏çË∂≥ÊÄé‰πàÊ∏ÖÁêÜ È©¨‰∏ä Á¥ßÊÄ•
        Á¶ªËÅåËµîÂÅøÊÄé‰πàÁÆó N+1 ÊòéÂ§©Ë∞à
        Â¶Ç‰ΩïÊ†πÊ≤ªÊó©Ëµ∑Âõ∞Èöæ ÊÄªÊòØÂ§±Ë¥•
        Ë£Ö‰øÆÈ¢ÑÁÆóÊéßÂà∂ÊñπÊ≥ï ËÄÅÊòØË∂ÖÊîØ
        """

    def _simulate_wechat_data(self) -> str:
        """Simulate WeChat search/article data."""
        return ""

    def _simulate_zhihu_data(self) -> str:
        """Simulate Zhihu Q&A data."""
        return """
        ‰∏∫‰ªÄ‰πàÊàëÊÄªÊòØÊó©Ëµ∑Â§±Ë¥•ÔºüÂ∞ùËØï‰∫ÜÂæàÂ§öÊñπÊ≥ïÈÉΩÊ≤°Áî®„ÄÇ
        ÊâãÊú∫Â≠òÂÇ®Á©∫Èó¥Êª°‰∫ÜÔºå‰∏çÊÉ≥Âà†ÁÖßÁâáÊÄé‰πàÂäûÔºü
        Á¶ªËÅåË∞àÂà§ÊúâÂì™‰∫õÊäÄÂ∑ßÔºüÊòéÂ§©Â∞±Ë¶ÅË∞à‰∫ÜÊå∫ÊÄ•ÁöÑ„ÄÇ
        """

    async def _fetch_local_content(self) -> str:
        """Fetch content from local sources and notes."""
        all_content = [self.text_content]

        for source_id in self.source_ids:
            try:
                source = await Source.get(source_id)
                if source and source.full_text:
                    all_content.append(source.full_text)
            except Exception as e:
                logger.warning(f"Failed to fetch source {source_id}: {e}")

        return "\n\n".join(filter(None, all_content))

    def _calculate_urgency_score(
        self,
        text: str,
        pain_type: PainPointType,
        platform: str
    ) -> int:
        """Calculate urgency score (0-100) based on painpoint type and indicators."""
        score = 50  # Base score
        text_lower = text.lower()

        if pain_type == PainPointType.INSTANT:
            # Time-sensitive keywords boost score
            for keyword in self.INSTANT_KEYWORDS:
                if keyword in text_lower:
                    score += 10
                    break  # Only count once

            # Help-seeking behavior indicates high urgency
            if any(word in text_lower for word in ["ÊÄé‰πàÂäû", "ÊïëÂëΩ", "help", "urgent"]):
                score += 15

            # Time pressure words
            if any(word in text_lower for word in ["‰ªäÊôö", "ÊòéÂ§©", "deadline", "due"]):
                score += 10

        elif pain_type == PainPointType.CONTINUOUS:
            # Frequency indicators
            for keyword in self.CONTINUOUS_KEYWORDS:
                if keyword in text_lower:
                    score += 8
                    break

            # Frustration level
            if any(word in text_lower for word in ["Âèó‰∏ç‰∫Ü", "Â¥©Ê∫É", "struggle", "frustrated"]):
                score += 12

        elif pain_type == PainPointType.HIDDEN:
            # Shame/hesitation indicators
            for keyword in self.HIDDEN_INDICATORS:
                if keyword in text_lower:
                    score += 10
                    break

            # Lower baseline for hidden painpoints
            score -= 10

        # Platform factor (some platforms indicate higher intent)
        platform_multiplier = {
            "baidu": 1.1,      # Search intent is high
            "zhihu": 1.05,     # Deep research intent
            "xiaohongshu": 1.0, # Discovery intent
            "douyin": 0.95,    # Entertainment intent
            "wechat": 1.0
        }
        score *= platform_multiplier.get(platform, 1.0)

        return min(int(score), 100)

    def _extract_keywords(self, text: str) -> List[str]:
        """Extract key terms from painpoint text."""
        # Simple keyword extraction (in production: use NLP)
        words = re.findall(r'\b\w{2,}\b', text)
        stop_words = {"the", "and", "for", "are", "but", "not", "you", "all", "can", "had", "her", "was", "one", "our", "out", "day", "get", "has", "him", "his", "how", "man", "new", "now", "old", "see", "two", "way", "who", "boy", "did", "its", "let", "put", "say", "she", "too", "use"}
        return [w for w in words if w.lower() not in stop_words][:5]

    def _detect_painpoints(self, text: str, platform: str) -> List[PainSignal]:
        """Detect all types of painpoints from text."""
        signals = []
        sentences = re.split(r'[.!?„ÄÇÔºÅÔºü\n]+', text)

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue

            sentence_lower = sentence.lower()

            # Detect instant painpoints
            if "instant" in self.pain_types:
                has_urgency = any(kw in sentence_lower for kw in self.INSTANT_KEYWORDS)
                has_help = any(word in sentence_lower for word in [
                    "ÊÄé‰πàÂäû", "Â¶Ç‰Ωï", "how to", "help", "Ê±ÇÂä©"
                ])

                if has_urgency or has_help:
                    urgency = self._calculate_urgency_score(sentence, PainPointType.INSTANT, platform)
                    if urgency >= self.min_urgency_score:
                        signals.append(PainSignal(
                            text=sentence[:300],
                            pain_type=PainPointType.INSTANT,
                            urgency_score=urgency,
                            source_platform=platform,
                            keywords=self._extract_keywords(sentence),
                            user_scenario=f"User needs immediate solution for: {sentence[:80]}...",
                            trend_direction="rising" if has_urgency else "stable"
                        ))

            # Detect continuous painpoints
            if "continuous" in self.pain_types:
                has_continuous = any(kw in sentence_lower for kw in self.CONTINUOUS_KEYWORDS)
                has_struggle = any(word in sentence_lower for word in [
                    "Âõ∞Èöæ", "struggle", "difficult", "hard to", "can\'t seem"
                ])

                if has_continuous or has_struggle:
                    urgency = self._calculate_urgency_score(sentence, PainPointType.CONTINUOUS, platform)
                    if urgency >= self.min_urgency_score:
                        signals.append(PainSignal(
                            text=sentence[:300],
                            pain_type=PainPointType.CONTINUOUS,
                            urgency_score=urgency,
                            source_platform=platform,
                            keywords=self._extract_keywords(sentence),
                            user_scenario=f"User has ongoing struggle with: {sentence[:80]}...",
                            trend_direction="stable"
                        ))

            # Detect hidden painpoints
            if "hidden" in self.pain_types:
                has_hidden = any(kw in sentence_lower for kw in self.HIDDEN_INDICATORS)

                if has_hidden:
                    urgency = self._calculate_urgency_score(sentence, PainPointType.HIDDEN, platform)
                    if urgency >= self.min_urgency_score:
                        signals.append(PainSignal(
                            text=sentence[:300],
                            pain_type=PainPointType.HIDDEN,
                            urgency_score=urgency,
                            source_platform=platform,
                            keywords=self._extract_keywords(sentence),
                            user_scenario=f"User has unspoken need: {sentence[:80]}...",
                            trend_direction="stable"
                        ))

        return signals

    async def _enhance_with_ai(self, signals: List[PainSignal]) -> List[PainSignal]:
        """Use AI to enhance painpoint detection and add insights."""
        if not signals:
            return signals

        try:
            from open_notebook.ai.provision import provision_langchain_model

            # Prepare signals for AI analysis
            signals_text = "\n".join([
                f"{i+1}. [{s.pain_type.value}] {s.text[:100]}"
                for i, s in enumerate(signals[:10])
            ])

            prompt = f"""Analyze these detected painpoints and provide:
1. Validation: Are these genuine painpoints?
2. Enhancement: Add related questions users might have
3. Trend analysis: Which are rising vs declining?

Painpoints:
{signals_text}

Return JSON:
{{
  "valid_signals": [0, 1, 2],
  "enhancements": [
    {{"index": 0, "related_questions": ["...", "..."], "trend": "rising"}}
  ]
}}"""

            # Note: In production, implement actual AI call
            # For now, return signals as-is with placeholder enhancement
            for signal in signals:
                signal.related_questions = [
                    f"How to solve {signal.keywords[0] if signal.keywords else 'this'}?",
                    f"Best way to deal with {signal.keywords[0] if signal.keywords else 'this'}?"
                ]

            return signals

        except Exception as e:
            logger.error(f"AI enhancement failed: {e}")
            return signals

    async def scan(self) -> List[PainSignal]:
        """Perform a full scan across all platforms."""
        logger.info(f"Starting painpoint scan across {len(self.platforms)} platforms...")

        all_signals = []

        # Fetch from external platforms
        for platform in self.platforms:
            try:
                data = await self._fetch_platform_data(platform)
                if data:
                    signals = self._detect_painpoints(data, platform)
                    all_signals.extend(signals)
                    logger.info(f"Detected {len(signals)} signals from {platform}")
            except Exception as e:
                logger.error(f"Error scanning {platform}: {e}")

        # Fetch from local content
        local_content = await self._fetch_local_content()
        if local_content:
            local_signals = self._detect_painpoints(local_content, "local")
            all_signals.extend(local_signals)
            logger.info(f"Detected {len(local_signals)} signals from local content")

        # Enhance with AI
        all_signals = await self._enhance_with_ai(all_signals)

        # Sort by urgency and deduplicate
        all_signals.sort(key=lambda x: x.urgency_score, reverse=True)
        unique_signals = self._deduplicate_signals(all_signals)

        # Store for later use
        self.detected_signals = unique_signals[:self.max_signals_per_scan]
        self.last_scan_time = datetime.utcnow()

        logger.info(f"Scan complete. Total unique signals: {len(self.detected_signals)}")
        return self.detected_signals

    def _deduplicate_signals(self, signals: List[PainSignal]) -> List[PainSignal]:
        """Remove duplicate signals based on text similarity."""
        unique = []
        seen_texts = set()

        for signal in signals:
            # Simple dedup: check if similar text already seen
            text_hash = signal.text[:50].lower().replace(" ", "")
            if text_hash not in seen_texts:
                seen_texts.add(text_hash)
                unique.append(signal)

        return unique

    def generate_daily_sync_report(self) -> DailySyncReport:
        """Generate report for daily sync with other P0 agents."""
        report = DailySyncReport(
            agent_name="PainScannerAgent",
            quadrant="Q1"
        )

        # Categorize signals
        instant_signals = [s for s in self.detected_signals if s.pain_type == PainPointType.INSTANT]
        continuous_signals = [s for s in self.detected_signals if s.pain_type == PainPointType.CONTINUOUS]
        hidden_signals = [s for s in self.detected_signals if s.pain_type == PainPointType.HIDDEN]

        # Add high-priority instant painpoints
        for signal in instant_signals[:3]:
            report.add_signal(signal.to_dict(), priority="high")

        # Add medium-priority continuous painpoints
        for signal in continuous_signals[:3]:
            report.add_signal(signal.to_dict(), priority="medium")

        # Add hidden painpoints (always interesting for cross-quadrant)
        for signal in hidden_signals[:2]:
            report.add_signal(signal.to_dict(), priority="high")

        # Generate cross-quadrant insights
        if instant_signals:
            report.add_insight(
                f"Q1 detected {len(instant_signals)} urgent painpoints. "
                f"Q2 should prepare emotional resonance content."
            )

        if hidden_signals:
            report.add_insight(
                f"Q1 found {len(hidden_signals)} hidden painpoints. "
                f"Q4 might use these for scenario-based awakening."
            )

        # Request collaborations
        if instant_signals:
            report.request_collaboration(
                target_quadrant="Q2",
                request_type="emotion_resonance",
                payload={
                    "painpoints": [s.to_dict() for s in instant_signals[:3]],
                    "needed": "emotional_storytelling"
                }
            )

        if hidden_signals:
            report.request_collaboration(
                target_quadrant="Q4",
                request_type="scenario_design",
                payload={
                    "painpoints": [s.to_dict() for s in hidden_signals[:2]],
                    "needed": "usage_scenarios"
                }
            )

        return report

    async def execute(self, context: SkillContext) -> SkillResult:
        """Execute the pain scanner agent.

        This is the main entry point when triggered via SkillRunner.
        """
        logger.info("PainScannerAgent (Q1P0) starting execution...")
        started_at = datetime.utcnow()

        try:
            # Perform scan
            signals = await self.scan()

            # Generate sync report if enabled
            sync_report = None
            if self.daily_sync_enabled:
                sync_report = self.generate_daily_sync_report()

            # Prepare output
            output = {
                "signals_detected": len(signals),
                "by_type": {
                    "instant": len([s for s in signals if s.pain_type == PainPointType.INSTANT]),
                    "continuous": len([s for s in signals if s.pain_type == PainPointType.CONTINUOUS]),
                    "hidden": len([s for s in signals if s.pain_type == PainPointType.HIDDEN])
                },
                "by_platform": {},
                "top_signals": [s.to_dict() for s in signals[:10]],
                "daily_sync_report": sync_report.to_dict() if sync_report else None,
                "last_scan": self.last_scan_time.isoformat() if self.last_scan_time else None
            }

            # Platform breakdown
            for signal in signals:
                platform = signal.source_platform
                output["by_platform"][platform] = output["by_platform"].get(platform, 0) + 1

            # Create result note if target notebook specified
            created_note_ids = []
            if self.config.target_notebook_id and signals:
                note_content = self._format_results(signals, output)
                result_note = Note(
                    title=f"[Q1P0] Pain Scanner Report - {datetime.now().strftime('%Y-%m-%d %H:%M')}",
                    content=note_content,
                    note_type="ai"
                )
                await result_note.save()
                await result_note.add_to_notebook(self.config.target_notebook_id)
                created_note_ids.append(str(result_note.id))
                output["created_note_id"] = str(result_note.id)

            return SkillResult(
                skill_id=context.skill_id,
                status=SkillStatus.SUCCESS,
                started_at=started_at,
                completed_at=datetime.utcnow(),
                output=output,
                created_note_ids=created_note_ids
            )

        except Exception as e:
            logger.exception(f"PainScannerAgent execution failed: {e}")
            return SkillResult(
                skill_id=context.skill_id,
                status=SkillStatus.FAILED,
                started_at=started_at,
                error_message=str(e)
            )

    def _format_results(self, signals: List[PainSignal], summary: Dict) -> str:
        """Format results as markdown for note output."""
        lines = [
            "# üîç Q1P0 Pain Scanner Agent Report\n",
            f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}",
            f"**Agent:** PainScannerAgent (Quadrant 1, Layer P0)",
            f"**Signals Detected:** {summary['signals_detected']}\n",
            "## Summary by Type\n",
            f"- ‚ö° Instant (Urgent): {summary['by_type']['instant']}",
            f"- üîÑ Continuous (Ongoing): {summary['by_type']['continuous']}",
            f"- ü´• Hidden (Unspoken): {summary['by_type']['hidden']}\n",
            "## Summary by Platform\n"
        ]

        for platform, count in summary['by_platform'].items():
            lines.append(f"- {platform}: {count} signals")

        lines.extend(["\n## Top Pain Signals\n"])

        for i, signal in enumerate(signals[:10], 1):
            emoji = {"instant": "‚ö°", "continuous": "üîÑ", "hidden": "ü´•"}.get(
                signal.pain_type.value, "üìç"
            )
            lines.extend([
                f"### {emoji} #{i} [{signal.pain_type.value.upper()}] Score: {signal.urgency_score}/100",
                f"**Signal:** {signal.text}",
                f"**Platform:** {signal.source_platform}",
                f"**User Scenario:** {signal.user_scenario}",
                f"**Keywords:** {', '.join(signal.keywords)}",
                f"**Trend:** {signal.trend_direction}",
                ""
            ])

        if summary.get('daily_sync_report'):
            report = summary['daily_sync_report']
            lines.extend([
                "\n## Daily Sync Report\n",
                f"**Agent:** {report['agent_name']}",
                f"**Quadrant:** {report['quadrant']}",
                f"**Signals to Share:** {report['signals_count']}\n",
                "### Cross-Quadrant Insights\n"
            ])
            for insight in report['insights']:
                lines.append(f"- {insight}")

        lines.extend([
            "\n---\n",
            "*This report was generated by the P0 Perception Layer Agent for Quadrant 1 (Intent Users).*",
            "*Next steps: Feed signals to Q1P1 (Value Judgment) for prioritization.*"
        ])

        return "\n".join(lines)


# =============================================================================
# Placeholder for other P0 Agents (to be implemented)
# =============================================================================

@register_skill
class EmotionWatcherAgent(Skill):
    """Q2P0 Agent: Monitors emotional signals in user interactions.

    Placeholder implementation - full version to be developed.
    """
    skill_type = "emotion_watcher_agent"
    name = "Emotion Watcher Agent (Q2P0)"
    description = "P0 perception agent that monitors emotional signals"

    parameters_schema = {
        "platforms": {"type": "array", "items": {"type": "string"}, "default": ["xiaohongshu", "douyin"]}
    }

    def __init__(self, config: SkillConfig):
        self.platforms = config.parameters.get("platforms", ["xiaohongshu"])
        super().__init__(config)

    async def execute(self, context: SkillContext) -> SkillResult:
        """Placeholder execution."""
        return SkillResult(
            skill_id=context.skill_id,
            status=SkillStatus.SUCCESS,
            started_at=datetime.utcnow(),
            output={"status": "placeholder", "message": "Q2P0 EmotionWatcherAgent - Full implementation pending"}
        )


@register_skill
class TrendHunterAgent(Skill):
    """Q3P0 Agent: Tracks trending topics across platforms.

    Placeholder implementation - full version to be developed.
    """
    skill_type = "trend_hunter_agent"
    name = "Trend Hunter Agent (Q3P0)"
    description = "P0 perception agent that tracks trending topics"

    parameters_schema = {
        "platforms": {"type": "array", "items": {"type": "string"}, "default": ["douyin", "weibo"]}
    }

    def __init__(self, config: SkillConfig):
        self.platforms = config.parameters.get("platforms", ["douyin"])
        super().__init__(config)

    async def execute(self, context: SkillContext) -> SkillResult:
        """Placeholder execution."""
        return SkillResult(
            skill_id=context.skill_id,
            status=SkillStatus.SUCCESS,
            started_at=datetime.utcnow(),
            output={"status": "placeholder", "message": "Q3P0 TrendHunterAgent - Full implementation pending"}
        )


@register_skill
class SceneDiscoverAgent(Skill):
    """Q4P0 Agent: Discovers potential usage scenarios.

    Placeholder implementation - full version to be developed.
    """
    skill_type = "scene_discover_agent"
    name = "Scene Discover Agent (Q4P0)"
    description = "P0 perception agent that discovers usage scenarios"

    parameters_schema = {
        "monitor_sources": {"type": "array", "items": {"type": "string"}, "default": []}
    }

    def __init__(self, config: SkillConfig):
        self.monitor_sources = config.parameters.get("monitor_sources", [])
        super().__init__(config)

    async def execute(self, context: SkillContext) -> SkillResult:
        """Placeholder execution."""
        return SkillResult(
            skill_id=context.skill_id,
            status=SkillStatus.SUCCESS,
            started_at=datetime.utcnow(),
            output={"status": "placeholder", "message": "Q4P0 SceneDiscoverAgent - Full implementation pending"}
        )
