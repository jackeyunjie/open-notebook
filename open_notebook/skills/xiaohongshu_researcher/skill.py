"""Xiaohongshu Researcher Skill.

Automatically searches and analyzes content about "solopreneur" topics
on Xiaohongshu (Little Red Book), extracts viral note patterns, and
saves findings to Notebook for further analysis.
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from loguru import logger
from playwright.async_api import Browser, Page, async_playwright

from open_notebook.database.repository import repo_create, repo_query


class XiaohongshuResearcherSkill:
    """Skill for researching solopreneur topics on Xiaohongshu."""

    def __init__(self):
        self.base_url = "https://www.xiaohongshu.com"
        self.search_url = "https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_search_result_notes"
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None

    async def initialize(self) -> None:
        """Initialize browser and page."""
        playwright = await async_playwright().start()
        self.browser = await playwright.chromium.launch(
            headless=False,  # Show browser for debugging
            args=[
                "--disable-blink-features=AutomationControlled",
                "--no-sandbox"
            ]
        )
        
        self.page = await self.browser.new_page(
            viewport={"width": 1920, "height": 1080}
        )
        
        # Set user agent to avoid detection
        await self.page.set_extra_http_headers({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        })

    async def close(self) -> None:
        """Close browser."""
        if self.browser:
            await self.browser.close()

    async def search_keyword(self, keyword: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """Search for a keyword and collect note data.
        
        Args:
            keyword: Search keyword
            max_results: Maximum notes to collect
            
        Returns:
            List of note data dictionaries
        """
        logger.info(f"Searching for: {keyword}")
        
        notes = []
        url = self.search_url.format(keyword=keyword)
        
        try:
            await self.page.goto(url, wait_until="networkidle")
            await asyncio.sleep(3)  # Wait for content to load
            
            # Scroll to load more results
            for _ in range(3):
                await self.page.evaluate("window.scrollBy(0, window.innerHeight)")
                await asyncio.sleep(2)
            
            # Extract note cards
            note_cards = await self.page.query_selector_all(".note-item")
            
            logger.info(f"Found {len(note_cards)} note cards")
            
            for i, card in enumerate(note_cards[:max_results]):
                try:
                    note_data = await self.extract_note_data(card)
                    if note_data:
                        notes.append(note_data)
                        logger.debug(f"Extracted note {i+1}: {note_data.get('title', 'N/A')[:50]}")
                except Exception as e:
                    logger.warning(f"Failed to extract note {i+1}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Search failed for {keyword}: {e}")
        
        return notes

    async def extract_note_data(self, card_element) -> Optional[Dict[str, Any]]:
        """Extract data from a single note card.
        
        Args:
            card_element: Playwright element handle for note card
            
        Returns:
            Dictionary with note data or None if extraction fails
        """
        try:
            # Extract title
            title_elem = await card_element.query_selector(".title")
            title = await title_elem.inner_text() if title_elem else ""
            
            # Extract author
            author_elem = await card_element.query_selector(".nickname")
            author = await author_elem.inner_text() if author_elem else ""
            
            # Extract engagement metrics
            like_elem = await card_element.query_selector(".icon-container >> text=ðŸ‘")
            like_count = await like_elem.inner_text() if like_elem else "0"
            
            collect_elem = await card_element.query_selector(".icon-container >> text=â­")
            collect_count = await collect_elem.inner_text() if collect_elem else "0"
            
            # Extract link
            link_elem = await card_element.query_selector("a.cover")
            link = await link_elem.get_attribute("href") if link_elem else ""
            
            # Clean up counts (remove commas, convert to int)
            like_count = int(like_count.replace(",", "").replace("ä¸‡", "0000"))
            collect_count = int(collect_count.replace(",", "").replace("ä¸‡", "0000"))
            
            return {
                "title": title.strip(),
                "author": author.strip(),
                "like_count": like_count,
                "collect_count": collect_count,
                "url": self.base_url + link if link.startswith("/") else link,
                "search_keyword": await card_element.evaluate("el => el.closest('[data-type=search]').dataset.keyword"),
                "collected_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.warning(f"Error extracting note data: {e}")
            return None

    async def analyze_patterns(self, notes: List[Dict[str, Any]]) -> List[str]:
        """Analyze collected notes to identify viral patterns.
        
        Args:
            notes: List of note data dictionaries
            
        Returns:
            List of insight strings
        """
        insights = []
        
        if not notes:
            return insights
        
        # Analyze title patterns
        titles = [n["title"] for n in notes if n.get("like_count", 0) > 100]
        if titles:
            # Check for numbers in titles
            num_with_digits = sum(1 for t in titles if any(c.isdigit() for c in t))
            if num_with_digits / len(titles) > 0.6:
                insights.append("çˆ†æ¬¾æ ‡é¢˜å¤šåŒ…å«æ•°å­—ï¼ˆ60% ä»¥ä¸Šé«˜èµžç¬”è®°ä½¿ç”¨ï¼‰")
            
            # Check for emotional words
            emotional_words = ["å¤ª", "è¶…", "çœŸçš„", "ç»äº†", "å¿…çœ‹", "å¹²è´§"]
            num_emotional = sum(1 for t in titles if any(w in t for w in emotional_words))
            if num_emotional / len(titles) > 0.5:
                insights.append("æƒ…ç»ªè¯ä½¿ç”¨é¢‘ç¹ï¼ˆ50% ä»¥ä¸Šæ ‡é¢˜åŒ…å«å¼ºçƒˆæƒ…æ„Ÿè¡¨è¾¾ï¼‰")
        
        # Analyze engagement distribution
        avg_likes = sum(n.get("like_count", 0) for n in notes) / len(notes)
        avg_collects = sum(n.get("collect_count", 0) for n in notes) / len(notes)
        
        insights.append(f"å¹³å‡ç‚¹èµžæ•°ï¼š{avg_likes:.0f}")
        insights.append(f"å¹³å‡æ”¶è—æ•°ï¼š{avg_collects:.0f}")
        
        # Collect-to-like ratio
        if avg_likes > 0:
            ratio = avg_collects / avg_likes
            if ratio > 0.3:
                insights.append(f"æ”¶è—/ç‚¹èµžæ¯”={ratio:.2f}ï¼Œå†…å®¹å®žç”¨æ€§å¼ºï¼ˆç”¨æˆ·å€¾å‘äºŽæ”¶è—ï¼‰")
        
        # Top authors
        author_counts = {}
        for note in notes:
            author = note.get("author", "Unknown")
            author_counts[author] = author_counts.get(author, 0) + 1
        
        top_authors = sorted(author_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        if top_authors:
            insights.append(f"æ´»è·ƒåˆ›ä½œè€…ï¼š{', '.join([a[0] for a in top_authors])}")
        
        return insights

    async def save_to_notebook(self, keyword: str, notes: List[Dict[str, Any]], insights: List[str]) -> int:
        """Save research results to Notebook.
        
        Args:
            keyword: Search keyword
            notes: List of note data
            insights: Analysis insights
            
        Returns:
            Number of sources created
        """
        today = datetime.now().strftime("%Y-%m-%d")
        notebook_name = f"å°çº¢ä¹¦ç ”ç©¶ - {today}"
        
        # Find or create notebook
        existing = await repo_query(
            f"SELECT * FROM notebook WHERE name = '{notebook_name}'"
        )
        
        if existing:
            notebook_id = existing[0]["id"]
        else:
            notebook_data = {
                "name": notebook_name,
                "description": f"è‡ªåŠ¨æ”¶é›†çš„å°çº¢ä¹¦'{keyword}'ç›¸å…³å†…å®¹ç ”ç©¶æ•°æ®",
                "created_at": datetime.now()
            }
            result = await repo_create("notebook", notebook_data)
            notebook_id = result["id"]
        
        # Create source with all notes
        source_data = {
            "notebook_id": notebook_id,
            "title": f"{keyword} - çˆ†æ¬¾ç¬”è®°åˆ†æž ({len(notes)}ç¯‡)",
            "source_type": "xiaohongshu_research",
            "url": f"https://www.xiaohongshu.com/search_result?keyword={keyword}",
            "content": json.dumps({
                "notes": notes,
                "insights": insights,
                "summary": {
                    "total_notes": len(notes),
                    "keyword": keyword,
                    "collected_at": datetime.now().isoformat()
                }
            }, ensure_ascii=False, indent=2),
            "metadata": {
                "research_type": "xiaohongshu",
                "keyword": keyword,
                "note_count": len(notes),
                "insight_count": len(insights)
            }
        }
        
        result = await repo_create("source", source_data)
        logger.info(f"Saved research to source: {result['id']}")
        
        return 1

    async def execute(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the research skill.
        
        Args:
            params: Execution parameters
                - keywords: List of keywords to search
                - max_results: Max results per keyword (default: 10)
                - save_to_notebook: Whether to save to notebook (default: True)
                
        Returns:
            Execution results
        """
        keywords = params.get("keywords", ["ä¸€äººå…¬å¸"])
        max_results = params.get("max_results", 10)
        save_to_notebook = params.get("save_to_notebook", True)
        
        logger.info(f"Starting Xiaohongshu research: {keywords}")
        
        await self.initialize()
        
        try:
            all_notes = []
            total_sources = 0
            
            for keyword in keywords:
                # Search and collect
                notes = await self.search_keyword(keyword, max_results)
                all_notes.extend(notes)
                
                # Analyze patterns
                insights = await self.analyze_patterns(notes)
                
                # Save to notebook
                if save_to_notebook and notes:
                    sources = await self.save_to_notebook(keyword, notes, insights)
                    total_sources += sources
                
                # Rate limiting
                if keyword != keywords[-1]:
                    await asyncio.sleep(3)
            
            return {
                "total_notes": len(all_notes),
                "sources_created": total_sources,
                "insights": await self.analyze_patterns(all_notes),
                "keywords_searched": keywords
            }
            
        finally:
            await self.close()


# Convenience function for direct usage
async def research_xiaohongshu(
    keywords: List[str] = ["ä¸€äººå…¬å¸"],
    max_results: int = 10,
    save_to_notebook: bool = True
) -> Dict[str, Any]:
    """Convenience function to run Xiaohongshu research.
    
    Args:
        keywords: Keywords to search
        max_results: Max results per keyword
        save_to_notebook: Whether to save to notebook
        
    Returns:
        Research results dictionary
    """
    skill = XiaohongshuResearcherSkill()
    return await skill.execute({
        "keywords": keywords,
        "max_results": max_results,
        "save_to_notebook": save_to_notebook
    })
