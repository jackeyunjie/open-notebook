"""P1 Layer Agents - Value Judgment Layer for Organic Growth System.

This module provides the P1 (Value Judgment Layer) agents that evaluate
and prioritize signals from the P0 perception layer.

Architecture:
    P0 Layer (Perception) â† Input from sensory agents
           â†“
    P1 Layer (Value Judgment) â† You are here
           â†“
    P2 Layer (Relationship)
           â†“
    P3 Layer (Evolution)

Four Quadrant Agents at P1:
    - Q1P1 PainpointValueAgent: Evaluates which painpoints are worth solving
    - Q2P1 EmotionAlignmentAgent: Validates emotional resonance effectiveness
    - Q3P1 TrendValueAgent: Assesses trend lifecycle and value
    - Q4P1 DemandAssessmentAgent: Evaluates potential demand conversion

Value Judgment Framework:
    Each P1 agent receives signals from its P0 counterpart and applies
    multi-dimensional value assessment to prioritize actions.
"""

import asyncio
import json
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

from loguru import logger

from open_notebook.domain.notebook import Note
from open_notebook.skills.base import Skill, SkillConfig, SkillContext, SkillResult, SkillStatus
from open_notebook.skills.p0_orchestrator import SharedMemory
from open_notebook.skills.registry import register_skill


class ValuePriority(Enum):
    """Priority levels after value assessment."""
    CRITICAL = "critical"      # Must act immediately
    HIGH = "high"             # Strong value proposition
    MEDIUM = "medium"         # Moderate value
    LOW = "low"               # Optional/Nice to have
    REJECT = "reject"         # Not worth pursuing


class ValueDimension(Enum):
    """Dimensions for value assessment."""
    URGENCY = "urgency"           # Time sensitivity
    IMPACT = "impact"             # Potential impact
    FEASIBILITY = "feasibility"   # Ease of execution
    ALIGNMENT = "alignment"       # Strategic fit
    TIMING = "timing"             # Market timing


@dataclass
class ValueAssessment:
    """Structured value assessment result."""
    signal_id: str
    source_quadrant: str
    overall_score: float  # 0-100
    priority: ValuePriority
    dimensions: Dict[ValueDimension, float]  # Per-dimension scores
    reasoning: str
    recommended_action: str
    confidence: float  # 0-1
    assessed_at: datetime = field(default_factory=datetime.utcnow)
    expires_at: Optional[datetime] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "signal_id": self.signal_id,
            "source_quadrant": self.source_quadrant,
            "overall_score": self.overall_score,
            "priority": self.priority.value,
            "dimensions": {k.value: v for k, v in self.dimensions.items()},
            "reasoning": self.reasoning,
            "recommended_action": self.recommended_action,
            "confidence": self.confidence,
            "assessed_at": self.assessed_at.isoformat(),
            "expires_at": self.expires_at.isoformat() if self.expires_at else None
        }


@dataclass
class ValueJudgmentReport:
    """Report generated by P1 agent after assessment."""
    agent_name: str
    quadrant: str
    layer: str = "P1"
    generated_at: datetime = field(default_factory=datetime.utcnow)
    assessments: List[ValueAssessment] = field(default_factory=list)
    prioritized_signals: List[str] = field(default_factory=list)
    insights: List[str] = field(default_factory=list)
    recommendations_for_p2: List[Dict[str, Any]] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "agent_name": self.agent_name,
            "quadrant": self.quadrant,
            "layer": self.layer,
            "generated_at": self.generated_at.isoformat(),
            "assessments_count": len(self.assessments),
            "assessments": [a.to_dict() for a in self.assessments],
            "prioritized_signals": self.prioritized_signals,
            "insights": self.insights,
            "recommendations_for_p2": self.recommendations_for_p2
        }


# =============================================================================
# Q1P1: Painpoint Value Agent
# =============================================================================

@register_skill
class PainpointValueAgent(Skill):
    """Q1P1 Agent: Evaluates which painpoints are worth solving.

    This agent receives painpoint signals from Q1P0 (PainScannerAgent) and applies
    multi-dimensional value assessment to determine which painpoints deserve
    content creation and solution development.

    Value Assessment Dimensions:
        1. Commercial Value (40%): Can we monetize this solution?
        2. Audience Size (25%): How many people have this pain?
        3. Competition (15%): Is the market saturated?
        4. Alignment (20%): Does it fit our expertise and positioning?

    Output:
        - Prioritized list of painpoints with value scores
        - Recommended content type for each painpoint
        - Estimation of conversion potential

    Example:
        config = SkillConfig(
            skill_type="painpoint_value_agent",
            name="Painpoint Value Agent Q1P1",
            parameters={
                "min_commercial_score": 60,
                "max_competition_threshold": 70,
                "target_notebook_id": "notebook:xxx"
            }
        )
    """

    skill_type = "painpoint_value_agent"
    name = "Painpoint Value Agent (Q1P1)"
    description = "P1 value judgment agent for evaluating painpoint signals"

    parameters_schema = {
        "signal_sources": {
            "type": "array",
            "items": {"type": "string"},
            "default": ["shared_memory"],
            "description": "Where to get P0 signals from"
        },
        "min_commercial_score": {
            "type": "integer",
            "default": 60,
            "minimum": 0,
            "maximum": 100,
            "description": "Minimum commercial viability score"
        },
        "max_competition_threshold": {
            "type": "integer",
            "default": 70,
            "minimum": 0,
            "maximum": 100,
            "description": "Maximum acceptable competition level"
        },
        "hours_lookback": {
            "type": "integer",
            "default": 24,
            "minimum": 1,
            "maximum": 168,
            "description": "How many hours back to look for signals"
        },
        "target_notebook_id": {
            "type": "string",
            "default": "",
            "description": "Notebook to store judgment report"
        }
    }

    def __init__(self, config: SkillConfig):
        self.signal_sources: List[str] = config.parameters.get("signal_sources", ["shared_memory"])
        self.min_commercial_score: int = config.parameters.get("min_commercial_score", 60)
        self.max_competition_threshold: int = config.parameters.get("max_competition_threshold", 70)
        self.hours_lookback: int = config.parameters.get("hours_lookback", 24)
        self.target_notebook_id: str = config.parameters.get("target_notebook_id", "")

        self.shared_memory = SharedMemory()
        self.assessments: List[ValueAssessment] = []

        super().__init__(config)

    async def _fetch_p0_signals(self) -> List[Dict[str, Any]]:
        """Fetch painpoint signals from P0 layer."""
        signals = []

        for source in self.signal_sources:
            if source == "shared_memory":
                # Get recent signals from SharedMemory
                recent = self.shared_memory.get_recent_signals(hours=self.hours_lookback)
                # Filter for Q1 signals (painpoints)
                pain_signals = [
                    s for s in recent
                    if s.get("source_quadrants") and "Q1" in s.get("source_quadrants", [])
                ]
                signals.extend(pain_signals)

        logger.info(f"Fetched {len(signals)} painpoint signals from P0 layer")
        return signals

    def _assess_commercial_value(self, signal: Dict[str, Any]) -> float:
        """Assess commercial viability (0-100)."""
        score = 50  # Base score

        # Urgency indicates willingness to pay
        urgency = signal.get("urgency_score", 50)
        score += (urgency - 50) * 0.3  # +/- 15 points

        # Pain type affects monetization
        pain_type = signal.get("pain_type", "")
        if pain_type == "instant":
            score += 20  # High urgency = easier to monetize
        elif pain_type == "continuous":
            score += 15  # Recurring need
        elif pain_type == "hidden":
            score += 10  # Requires education

        # Volume estimate (if available)
        volume = signal.get("volume_estimate", 0)
        if volume > 10000:
            score += 10
        elif volume > 1000:
            score += 5

        return min(max(score, 0), 100)

    def _assess_audience_size(self, signal: Dict[str, Any]) -> float:
        """Estimate audience size (0-100)."""
        score = 50

        # Platform reach
        platform = signal.get("source_platform", "")
        platform_multipliers = {
            "baidu": 1.2,      # Search = high intent
            "douyin": 1.3,     # Mass market
            "xiaohongshu": 1.1, # Niche but engaged
            "zhihu": 0.9,      # Smaller but high value
            "wechat": 1.0
        }
        score *= platform_multipliers.get(platform, 1.0)

        # Search volume proxy
        volume = signal.get("volume_estimate", 0)
        if volume > 50000:
            score += 25
        elif volume > 10000:
            score += 15
        elif volume > 1000:
            score += 5

        return min(score, 100)

    def _assess_competition(self, signal: Dict[str, Any]) -> float:
        """Assess competition level (0-100, higher = more competition)."""
        # In production: analyze existing content, competitors
        # For now: use heuristics

        score = 50  # Medium competition default

        # Generic topics have more competition
        keywords = signal.get("keywords", [])
        generic_terms = ["how to", "tips", "guide", "best"]
        if any(term in " ".join(keywords).lower() for term in generic_terms):
            score += 15

        # Very specific painpoints have less competition
        pain_text = signal.get("text", "")
        if len(pain_text) > 100 and any(word in pain_text for word in ["specific", "exactly"]):
            score -= 10

        return min(max(score, 0), 100)

    def _assess_alignment(self, signal: Dict[str, Any]) -> float:
        """Assess strategic alignment (0-100)."""
        # In production: compare against IP positioning, expertise areas
        # For now: use signal quality as proxy

        score = 60  # Default reasonable alignment

        # Signal quality indicators
        if signal.get("trend_direction") == "rising":
            score += 10

        if len(signal.get("related_questions", [])) > 3:
            score += 10  # Deep interest

        # Hidden painpoints often indicate underserved niches
        if signal.get("pain_type") == "hidden":
            score += 10

        return min(score, 100)

    def _assess_painpoint(self, signal: Dict[str, Any]) -> ValueAssessment:
        """Perform full multi-dimensional assessment of a painpoint."""
        signal_id = signal.get("signal_id", str(hash(str(signal))))

        # Calculate dimension scores
        commercial = self._assess_commercial_value(signal)
        audience = self._assess_audience_size(signal)
        competition = self._assess_competition(signal)
        alignment = self._assess_alignment(signal)

        # Weighted overall score
        weights = {
            "commercial": 0.40,
            "audience": 0.25,
            "competition_inv": 0.15,  # Inverted - lower competition is better
            "alignment": 0.20
        }

        # Invert competition score (lower competition = higher value)
        competition_inv = 100 - competition

        overall_score = (
            commercial * weights["commercial"] +
            audience * weights["audience"] +
            competition_inv * weights["competition_inv"] +
            alignment * weights["alignment"]
        )

        # Determine priority
        if overall_score >= 85 and commercial >= 70:
            priority = ValuePriority.CRITICAL
        elif overall_score >= 70:
            priority = ValuePriority.HIGH
        elif overall_score >= 50:
            priority = ValuePriority.MEDIUM
        elif overall_score >= 30:
            priority = ValuePriority.LOW
        else:
            priority = ValuePriority.REJECT

        # Generate reasoning
        reasoning_parts = []
        if commercial >= 70:
            reasoning_parts.append("High commercial potential")
        if audience >= 70:
            reasoning_parts.append("Large addressable audience")
        if competition < 50:
            reasoning_parts.append("Low competition")
        if alignment >= 70:
            reasoning_parts.append("Strong strategic alignment")

        reasoning = "; ".join(reasoning_parts) if reasoning_parts else "Moderate overall value"

        # Recommended action
        if priority == ValuePriority.CRITICAL:
            action = "Create immediate response content + develop solution offering"
        elif priority == ValuePriority.HIGH:
            action = "Add to content queue with priority scheduling"
        elif priority == ValuePriority.MEDIUM:
            action = "Consider for content calendar"
        elif priority == ValuePriority.LOW:
            action = "Monitor for future opportunities"
        else:
            action = "Decline - insufficient value"

        return ValueAssessment(
            signal_id=signal_id,
            source_quadrant="Q1",
            overall_score=overall_score,
            priority=priority,
            dimensions={
                ValueDimension.URGENCY: signal.get("urgency_score", 50),
                ValueDimension.IMPACT: commercial,
                ValueDimension.FEASIBILITY: 100 - competition,  # Lower competition = higher feasibility
                ValueDimension.ALIGNMENT: alignment,
                ValueDimension.TIMING: 80 if signal.get("trend_direction") == "rising" else 50
            },
            reasoning=reasoning,
            recommended_action=action,
            confidence=0.7 + (overall_score / 1000)  # 0.7-0.8 range
        )

    async def execute(self, context: SkillContext) -> SkillResult:
        """Execute Q1P1 value judgment."""
        started_at = datetime.utcnow()

        try:
            # Fetch P0 signals
            signals = await self._fetch_p0_signals()

            if not signals:
                return SkillResult(
                    skill_id=context.skill_id,
                    status=SkillStatus.SUCCESS,
                    started_at=started_at,
                    output={
                        "message": "No painpoint signals found from P0 layer",
                        "assessments": [],
                        "recommendations": []
                    }
                )

            # Assess each signal
            self.assessments = []
            for signal in signals:
                assessment = self._assess_painpoint(signal)
                if assessment.priority != ValuePriority.REJECT:
                    self.assessments.append(assessment)

            # Sort by overall score
            self.assessments.sort(key=lambda x: x.overall_score, reverse=True)

            # Generate report
            report = ValueJudgmentReport(
                agent_name="PainpointValueAgent",
                quadrant="Q1"
            )
            report.assessments = self.assessments
            report.prioritized_signals = [a.signal_id for a in self.assessments[:10]]

            # Generate insights
            critical_count = sum(1 for a in self.assessments if a.priority == ValuePriority.CRITICAL)
            high_count = sum(1 for a in self.assessments if a.priority == ValuePriority.HIGH)

            if critical_count > 0:
                report.insights.append(
                    f"Detected {critical_count} CRITICAL painpoints requiring immediate action"
                )

            if high_count >= 3:
                report.insights.append(
                    f"Strong pipeline: {high_count} high-value opportunities identified"
                )

            # Recommendations for P2 layer
            for assessment in self.assessments[:5]:
                report.recommendations_for_p2.append({
                    "signal_id": assessment.signal_id,
                    "action": "develop_trust",
                    "priority": assessment.priority.value,
                    "rationale": f"High value painpoint ({assessment.overall_score:.0f}/100)"
                })

            # Store assessments to SharedMemory for upper layers
            for assessment in self.assessments:
                key = f"p1_assessment:{assessment.signal_id}"
                self.shared_memory.store(key, assessment.to_dict(), ttl_seconds=86400)

            # Prepare output
            output = {
                "total_signals_assessed": len(signals),
                "qualified_assessments": len(self.assessments),
                "by_priority": {
                    "critical": critical_count,
                    "high": high_count,
                    "medium": sum(1 for a in self.assessments if a.priority == ValuePriority.MEDIUM),
                    "low": sum(1 for a in self.assessments if a.priority == ValuePriority.LOW)
                },
                "top_opportunities": [a.to_dict() for a in self.assessments[:5]],
                "report": report.to_dict()
            }

            # Create note if target notebook specified
            created_note_ids = []
            if self.target_notebook_id:
                note_content = self._format_report(report)
                result_note = Note(
                    title=f"[Q1P1] Painpoint Value Assessment - {datetime.now().strftime('%Y-%m-%d')}",
                    content=note_content,
                    note_type="ai"
                )
                await result_note.save()
                await result_note.add_to_notebook(self.target_notebook_id)
                created_note_ids.append(str(result_note.id))

            return SkillResult(
                skill_id=context.skill_id,
                status=SkillStatus.SUCCESS,
                started_at=started_at,
                completed_at=datetime.utcnow(),
                output=output,
                created_note_ids=created_note_ids
            )

        except Exception as e:
            logger.exception(f"Q1P1 execution failed: {e}")
            return SkillResult(
                skill_id=context.skill_id,
                status=SkillStatus.FAILED,
                started_at=started_at,
                error_message=str(e)
            )

    def _format_report(self, report: ValueJudgmentReport) -> str:
        """Format report as markdown."""
        lines = [
            "# ðŸŽ¯ Q1P1 Painpoint Value Assessment Report\n",
            f"**Agent:** {report.agent_name}",
            f"**Layer:** P1 (Value Judgment)",
            f"**Generated:** {report.generated_at.strftime('%Y-%m-%d %H:%M:%S')}\n",
            f"**Total Assessments:** {len(report.assessments)}\n",
            "## Top Value Opportunities\n"
        ]

        for i, assessment in enumerate(report.assessments[:10], 1):
            emoji = {
                ValuePriority.CRITICAL: "ðŸ”´",
                ValuePriority.HIGH: "ðŸŸ ",
                ValuePriority.MEDIUM: "ðŸŸ¡",
                ValuePriority.LOW: "âšª"
            }.get(assessment.priority, "âšª")

            lines.extend([
                f"### {emoji} #{i} Score: {assessment.overall_score:.0f}/100",
                f"**Priority:** {assessment.priority.value.upper()}",
                f"**Reasoning:** {assessment.reasoning}",
                f"**Action:** {assessment.recommended_action}",
                "**Dimension Scores:**"
            ])

            for dim, score in assessment.dimensions.items():
                lines.append(f"  - {dim.value}: {score:.0f}/100")

            lines.append("")

        if report.insights:
            lines.extend(["\n## Strategic Insights\n"])
            for insight in report.insights:
                lines.append(f"- ðŸ’¡ {insight}")

        lines.extend([
            "\n---\n",
            "*This assessment was generated by the P1 Value Judgment Layer.*",
            "*Next: Route to P2 (Relationship Layer) for trust building.*"
        ])

        return "\n".join(lines)


# =============================================================================
# Placeholder P1 Agents (to be fully implemented)
# =============================================================================

@register_skill
class EmotionAlignmentAgent(Skill):
    """Q2P1 Agent: Validates emotional resonance effectiveness.

    This agent receives emotional signals from Q2P0 (EmotionWatcherAgent) and evaluates
    which emotional narratives will create genuine resonance with the target audience.

    Value Assessment Dimensions:
        1. Authenticity (30%): Is the emotion genuine vs. manipulative?
        2. Audience Match (25%): Does it resonate with our specific audience?
        3. Shareability (20%): Will people want to share this emotion?
        4. Brand Alignment (15%): Does it fit our brand voice?
        5. Conversion Potential (10%): Can it drive desired actions?

    Output:
        - Validated emotional narratives with resonance scores
        - Recommended storytelling approaches
        - Risk assessment (avoiding emotional manipulation)
    """
    skill_type = "emotion_alignment_agent"
    name = "Emotion Alignment Agent (Q2P1)"
    description = "P1 value judgment agent for validating emotional resonance"

    parameters_schema = {
        "signal_sources": {
            "type": "array",
            "items": {"type": "string"},
            "default": ["shared_memory"],
            "description": "Where to get Q2P0 signals from"
        },
        "min_resonance_score": {
            "type": "integer",
            "default": 60,
            "minimum": 0,
            "maximum": 100
        },
        "min_authenticity_score": {
            "type": "integer",
            "default": 70,
            "minimum": 0,
            "maximum": 100,
            "description": "Minimum authenticity to avoid manipulation"
        },
        "hours_lookback": {
            "type": "integer",
            "default": 24,
            "minimum": 1,
            "maximum": 168
        },
        "target_notebook_id": {
            "type": "string",
            "default": ""
        }
    }

    def __init__(self, config: SkillConfig):
        self.signal_sources = config.parameters.get("signal_sources", ["shared_memory"])
        self.min_resonance_score = config.parameters.get("min_resonance_score", 60)
        self.min_authenticity_score = config.parameters.get("min_authenticity_score", 70)
        self.hours_lookback = config.parameters.get("hours_lookback", 24)
        self.target_notebook_id = config.parameters.get("target_notebook_id", "")
        self.shared_memory = SharedMemory()
        self.assessments: List[ValueAssessment] = []
        super().__init__(config)

    async def _fetch_q2p0_signals(self) -> List[Dict[str, Any]]:
        """Fetch emotional signals from Q2P0 layer."""
        signals = []
        for source in self.signal_sources:
            if source == "shared_memory":
                recent = self.shared_memory.get_recent_signals(hours=self.hours_lookback)
                emotion_signals = [
                    s for s in recent
                    if s.get("source_quadrants") and "Q2" in s.get("source_quadrants", [])
                ]
                signals.extend(emotion_signals)
        logger.info(f"Fetched {len(signals)} emotional signals from Q2P0")
        return signals

    def _assess_authenticity(self, signal: Dict[str, Any]) -> float:
        """Assess emotional authenticity (0-100)."""
        score = 60  # Base score
        emotion_type = signal.get("emotion", "").lower()
        text = signal.get("text", "").lower()

        # Positive authenticity markers
        authentic_markers = ["i feel", "my experience", "personally", "honestly", "genuinely"]
        score += sum(10 for marker in authentic_markers if marker in text)

        # Negative markers (potential manipulation)
        manipulative_markers = ["everyone is", "you must", "don't you", "how dare"]
        score -= sum(15 for marker in manipulative_markers if marker in text)

        # Emotion type adjustments
        genuine_emotions = ["frustration", "joy", "gratitude", "anxiety"]
        performative_emotions = ["outrage", "hype", "fomo"]
        if emotion_type in genuine_emotions:
            score += 10
        elif emotion_type in performative_emotions:
            score -= 5

        return min(max(score, 0), 100)

    def _assess_audience_match(self, signal: Dict[str, Any]) -> float:
        """Assess match with target audience (0-100)."""
        score = 65  # Default moderate match
        intensity = signal.get("intensity", 50)

        # High intensity emotions resonate more
        if intensity > 80:
            score += 15
        elif intensity > 60:
            score += 5

        # Platform indicates audience type
        platform = signal.get("source_platform", "")
        if platform in ["xiaohongshu", "douyin"]:
            score += 10  # Visual/younger audience

        return min(score, 100)

    def _assess_shareability(self, signal: Dict[str, Any]) -> float:
        """Assess likelihood of being shared (0-100)."""
        score = 50
        emotion_type = signal.get("emotion", "").lower()

        # High-arousal emotions spread more
        high_arousal = ["anger", "joy", "awe", "anxiety"]
        if emotion_type in high_arousal:
            score += 25

        # Relatability indicators
        text = signal.get("text", "")
        if any(word in text for word in ["we all", "everyone", "anyone else"]):
            score += 15

        return min(score, 100)

    def _assess_brand_alignment(self, signal: Dict[str, Any]) -> float:
        """Assess alignment with brand voice (0-100)."""
        # In production: compare against brand guidelines
        score = 70  # Default good alignment

        emotion_type = signal.get("emotion", "").lower()
        # Professional brands avoid extreme emotions
        if emotion_type in ["outrage", "hysteria"]:
            score -= 20

        return max(score, 0)

    def _assess_emotion(self, signal: Dict[str, Any]) -> ValueAssessment:
        """Perform full assessment of an emotional signal."""
        signal_id = signal.get("signal_id", str(hash(str(signal))))

        authenticity = self._assess_authenticity(signal)
        audience_match = self._assess_audience_match(signal)
        shareability = self._assess_shareability(signal)
        brand_alignment = self._assess_brand_alignment(signal)

        # Weighted overall score
        weights = {"authenticity": 0.30, "audience": 0.25, "share": 0.20, "brand": 0.15, "conversion": 0.10}

        conversion_potential = (shareability + audience_match) / 2
        overall_score = (
            authenticity * weights["authenticity"] +
            audience_match * weights["audience"] +
            shareability * weights["share"] +
            brand_alignment * weights["brand"] +
            conversion_potential * weights["conversion"]
        )

        # Determine priority (reject if not authentic enough)
        if authenticity < self.min_authenticity_score:
            priority = ValuePriority.REJECT
        elif overall_score >= 80:
            priority = ValuePriority.CRITICAL
        elif overall_score >= 65:
            priority = ValuePriority.HIGH
        elif overall_score >= self.min_resonance_score:
            priority = ValuePriority.MEDIUM
        else:
            priority = ValuePriority.LOW

        # Reasoning
        reasoning_parts = []
        if authenticity >= 75:
            reasoning_parts.append("High authenticity")
        if shareability >= 70:
            reasoning_parts.append("Strong viral potential")
        if audience_match >= 70:
            reasoning_parts.append("Good audience fit")

        reasoning = "; ".join(reasoning_parts) if reasoning_parts else "Moderate emotional resonance"

        # Recommended action
        if priority == ValuePriority.CRITICAL:
            action = "Create flagship emotional narrative content"
        elif priority == ValuePriority.HIGH:
            action = "Develop emotional storytelling series"
        elif priority == ValuePriority.MEDIUM:
            action = "Consider for community engagement content"
        elif priority == ValuePriority.LOW:
            action = "Monitor emotional trends"
        else:
            action = "Reject - insufficient authenticity or manipulation risk"

        return ValueAssessment(
            signal_id=signal_id,
            source_quadrant="Q2",
            overall_score=overall_score,
            priority=priority,
            dimensions={
                ValueDimension.URGENCY: signal.get("intensity", 50),
                ValueDimension.IMPACT: shareability,
                ValueDimension.FEASIBILITY: authenticity,
                ValueDimension.ALIGNMENT: brand_alignment,
                ValueDimension.TIMING: 70 if signal.get("trend_direction") == "rising" else 50
            },
            reasoning=reasoning,
            recommended_action=action,
            confidence=0.75
        )

    async def execute(self, context: SkillContext) -> SkillResult:
        """Execute Q2P1 emotional alignment assessment."""
        started_at = datetime.utcnow()

        try:
            signals = await self._fetch_q2p0_signals()
            if not signals:
                return SkillResult(
                    skill_id=context.skill_id,
                    status=SkillStatus.SUCCESS,
                    started_at=started_at,
                    output={"message": "No emotional signals found", "assessments": []}
                )

            self.assessments = []
            for signal in signals:
                assessment = self._assess_emotion(signal)
                if assessment.priority != ValuePriority.REJECT:
                    self.assessments.append(assessment)

            self.assessments.sort(key=lambda x: x.overall_score, reverse=True)

            # Generate report
            report = ValueJudgmentReport(agent_name="EmotionAlignmentAgent", quadrant="Q2")
            report.assessments = self.assessments
            report.prioritized_signals = [a.signal_id for a in self.assessments[:10]]

            high_count = sum(1 for a in self.assessments if a.priority == ValuePriority.HIGH)
            if high_count >= 3:
                report.insights.append(f"Strong emotional resonance opportunities: {high_count} validated")

            # Store to SharedMemory
            for assessment in self.assessments:
                key = f"p1_assessment:q2:{assessment.signal_id}"
                self.shared_memory.store(key, assessment.to_dict(), ttl_seconds=86400)

            output = {
                "total_signals": len(signals),
                "qualified": len(self.assessments),
                "top_emotions": [a.to_dict() for a in self.assessments[:5]],
                "report": report.to_dict()
            }

            created_note_ids = []
            if self.target_notebook_id:
                note = Note(
                    title=f"[Q2P1] Emotion Alignment - {datetime.now().strftime('%Y-%m-%d')}",
                    content=self._format_report(report),
                    note_type="ai"
                )
                await note.save()
                await note.add_to_notebook(self.target_notebook_id)
                created_note_ids.append(str(note.id))

            return SkillResult(
                skill_id=context.skill_id,
                status=SkillStatus.SUCCESS,
                started_at=started_at,
                completed_at=datetime.utcnow(),
                output=output,
                created_note_ids=created_note_ids
            )

        except Exception as e:
            logger.exception(f"Q2P1 execution failed: {e}")
            return SkillResult(skill_id=context.skill_id, status=SkillStatus.FAILED,
                             started_at=started_at, error_message=str(e))

    def _format_report(self, report: ValueJudgmentReport) -> str:
        lines = ["# ðŸ’ Q2P1 Emotion Alignment Report\n", f"**Assessments:** {len(report.assessments)}\n"]
        for i, a in enumerate(report.assessments[:10], 1):
            emoji = {"critical": "ðŸ”´", "high": "ðŸŸ ", "medium": "ðŸŸ¡", "low": "âšª"}.get(a.priority.value, "âšª")
            lines.extend([f"### {emoji} #{i} {a.priority.value.upper()} - Score: {a.overall_score:.0f}",
                         f"**Reasoning:** {a.reasoning}", f"**Action:** {a.recommended_action}", ""])
        return "\n".join(lines)


@register_skill
class TrendValueAgent(Skill):
    """Q3P1 Agent: Assesses trend lifecycle and value.

    This agent receives trend signals from Q3P0 (TrendHunterAgent) and evaluates
    which trends are worth riding (high value, right timing) vs. ignoring
    (peaked, irrelevant, or misaligned).

    Value Assessment Dimensions:
        1. Lifecycle Stage (30%): Early growth vs. peaking vs. declining
        2. Relevance Fit (25%): How well it fits our domain/expertise
        3. Sustainability (20%): Will it last or fade quickly?
        4. Entry Cost (15%): How much effort to participate?
        5. Viral Potential (10%): Can we ride it to broad awareness?

    Output:
        - Trend lifecycle assessments with timing recommendations
        - Entry window predictions
        - Risk assessment for each trend
    """
    skill_type = "trend_value_agent"
    name = "Trend Value Agent (Q3P1)"
    description = "P1 value judgment agent for trend assessment"

    parameters_schema = {
        "signal_sources": {
            "type": "array",
            "items": {"type": "string"},
            "default": ["shared_memory"]
        },
        "min_trend_score": {
            "type": "integer",
            "default": 65,
            "minimum": 0,
            "maximum": 100
        },
        "hours_lookback": {
            "type": "integer",
            "default": 24,
            "minimum": 1,
            "maximum": 168
        },
        "target_notebook_id": {
            "type": "string",
            "default": ""
        }
    }

    def __init__(self, config: SkillConfig):
        self.signal_sources = config.parameters.get("signal_sources", ["shared_memory"])
        self.min_trend_score = config.parameters.get("min_trend_score", 65)
        self.hours_lookback = config.parameters.get("hours_lookback", 24)
        self.target_notebook_id = config.parameters.get("target_notebook_id", "")
        self.shared_memory = SharedMemory()
        self.assessments: List[ValueAssessment] = []
        super().__init__(config)

    async def _fetch_q3p0_signals(self) -> List[Dict[str, Any]]:
        """Fetch trend signals from Q3P0 layer."""
        signals = []
        for source in self.signal_sources:
            if source == "shared_memory":
                recent = self.shared_memory.get_recent_signals(hours=self.hours_lookback)
                trend_signals = [
                    s for s in recent
                    if s.get("source_quadrants") and "Q3" in s.get("source_quadrants", [])
                ]
                signals.extend(trend_signals)
        logger.info(f"Fetched {len(signals)} trend signals from Q3P0")
        return signals

    def _assess_lifecycle_stage(self, signal: Dict[str, Any]) -> Tuple[float, str]:
        """Assess trend lifecycle stage, return (score, stage_name)."""
        growth_rate = signal.get("growth_rate", "0%")
        lifecycle = signal.get("lifecycle", "stable")

        # Parse growth rate
        try:
            growth = float(growth_rate.replace("%", "").replace("+", ""))
        except:
            growth = 0

        # Score based on lifecycle
        if lifecycle == "rising":
            if growth > 200:
                return (90, "early_growth")  # Sweet spot
            elif growth > 100:
                return (85, "accelerating")
            else:
                return (75, "growing")
        elif lifecycle == "peak":
            return (50, "peaking")  # Risky entry
        elif lifecycle == "falling":
            return (20, "declining")  # Too late
        else:
            return (60, "stable")

    def _assess_relevance_fit(self, signal: Dict[str, Any]) -> float:
        """Assess how well trend fits our domain (0-100)."""
        score = 60  # Default moderate fit

        # Heat score indicates relevance (platform algorithm)
        heat = signal.get("heat_score", 50)
        score += (heat - 50) * 0.3

        # Topic keywords analysis
        topic = signal.get("topic", "")
        # In production: compare against domain keywords
        generic_topics = ["news", "weather", "celebrity"]
        if any(t in topic.lower() for t in generic_topics):
            score -= 15  # Less relevant to niche

        return min(max(score, 0), 100)

    def _assess_sustainability(self, signal: Dict[str, Any]) -> float:
        """Assess trend sustainability (0-100)."""
        score = 50

        lifecycle = signal.get("lifecycle", "")
        if lifecycle == "rising":
            score += 25

        # Topic-based sustainability
        topic = signal.get("topic", "").lower()
        fleeting_indicators = ["challenge", "viral", "meme", "today"]
        sustainable_indicators = ["guide", "method", "system", "framework"]

        score -= sum(10 for w in fleeting_indicators if w in topic)
        score += sum(10 for w in sustainable_indicators if w in topic)

        return min(max(score, 0), 100)

    def _assess_entry_cost(self, signal: Dict[str, Any]) -> float:
        """Assess effort required to enter trend (0-100, higher = lower cost)."""
        score = 70  # Default reasonable cost

        # Action window indicates urgency/cost
        window = signal.get("action_window", "")
        if "24h" in window or "now" in window:
            score -= 20  # High urgency = higher cost

        return max(score, 0)

    def _assess_trend(self, signal: Dict[str, Any]) -> ValueAssessment:
        """Perform full assessment of a trend signal."""
        signal_id = signal.get("signal_id", str(hash(str(signal))))

        lifecycle_score, stage = self._assess_lifecycle_stage(signal)
        relevance = self._assess_relevance_fit(signal)
        sustainability = self._assess_sustainability(signal)
        entry_cost = self._assess_entry_cost(signal)

        # Viral potential based on platform and heat
        viral = (signal.get("heat_score", 50) + signal.get("relevance_to_ip", 50)) / 2

        # Weighted overall score
        weights = {"lifecycle": 0.30, "relevance": 0.25, "sustainability": 0.20, "cost": 0.15, "viral": 0.10}
        overall_score = (
            lifecycle_score * weights["lifecycle"] +
            relevance * weights["relevance"] +
            sustainability * weights["sustainability"] +
            entry_cost * weights["cost"] +
            viral * weights["viral"]
        )

        # Determine priority
        if lifecycle_score >= 80 and relevance >= 70:
            priority = ValuePriority.CRITICAL
        elif overall_score >= 75:
            priority = ValuePriority.HIGH
        elif overall_score >= self.min_trend_score:
            priority = ValuePriority.MEDIUM
        elif overall_score >= 40:
            priority = ValuePriority.LOW
        else:
            priority = ValuePriority.REJECT

        # Reasoning
        reasoning_parts = []
        if lifecycle_score >= 80:
            reasoning_parts.append(f"Optimal lifecycle stage: {stage}")
        if relevance >= 70:
            reasoning_parts.append("High domain relevance")
        if sustainability >= 70:
            reasoning_parts.append("Sustainable trend")

        reasoning = "; ".join(reasoning_parts) if reasoning_parts else f"Trend in {stage} stage"

        # Recommended action
        if priority == ValuePriority.CRITICAL:
            action = f"URGENT: Enter trend immediately (stage: {stage})"
        elif priority == ValuePriority.HIGH:
            action = f"Create trend-responsive content within 24-48h"
        elif priority == ValuePriority.MEDIUM:
            action = f"Monitor and prepare content for {stage} trend"
        elif priority == ValuePriority.LOW:
            action = "Observe trend development"
        else:
            action = f"Skip - trend is {stage}"

        return ValueAssessment(
            signal_id=signal_id,
            source_quadrant="Q3",
            overall_score=overall_score,
            priority=priority,
            dimensions={
                ValueDimension.URGENCY: 100 if stage == "early_growth" else 50,
                ValueDimension.IMPACT: viral,
                ValueDimension.FEASIBILITY: entry_cost,
                ValueDimension.ALIGNMENT: relevance,
                ValueDimension.TIMING: lifecycle_score
            },
            reasoning=reasoning,
            recommended_action=action,
            confidence=0.72
        )

    async def execute(self, context: SkillContext) -> SkillResult:
        """Execute Q3P1 trend value assessment."""
        started_at = datetime.utcnow()

        try:
            signals = await self._fetch_q3p0_signals()
            if not signals:
                return SkillResult(
                    skill_id=context.skill_id,
                    status=SkillStatus.SUCCESS,
                    started_at=started_at,
                    output={"message": "No trend signals found", "assessments": []}
                )

            self.assessments = []
            for signal in signals:
                assessment = self._assess_trend(signal)
                if assessment.priority != ValuePriority.REJECT:
                    self.assessments.append(assessment)

            self.assessments.sort(key=lambda x: x.overall_score, reverse=True)

            # Generate report
            report = ValueJudgmentReport(agent_name="TrendValueAgent", quadrant="Q3")
            report.assessments = self.assessments
            report.prioritized_signals = [a.signal_id for a in self.assessments[:10]]

            critical_count = sum(1 for a in self.assessments if a.priority == ValuePriority.CRITICAL)
            if critical_count > 0:
                report.insights.append(f"{critical_count} trends require IMMEDIATE entry")

            # Store to SharedMemory
            for assessment in self.assessments:
                key = f"p1_assessment:q3:{assessment.signal_id}"
                self.shared_memory.store(key, assessment.to_dict(), ttl_seconds=86400)

            output = {
                "total_signals": len(signals),
                "qualified": len(self.assessments),
                "by_stage": {},
                "top_trends": [a.to_dict() for a in self.assessments[:5]],
                "report": report.to_dict()
            }

            created_note_ids = []
            if self.target_notebook_id:
                note = Note(
                    title=f"[Q3P1] Trend Value - {datetime.now().strftime('%Y-%m-%d')}",
                    content=self._format_report(report),
                    note_type="ai"
                )
                await note.save()
                await note.add_to_notebook(self.target_notebook_id)
                created_note_ids.append(str(note.id))

            return SkillResult(
                skill_id=context.skill_id,
                status=SkillStatus.SUCCESS,
                started_at=started_at,
                completed_at=datetime.utcnow(),
                output=output,
                created_note_ids=created_note_ids
            )

        except Exception as e:
            logger.exception(f"Q3P1 execution failed: {e}")
            return SkillResult(skill_id=context.skill_id, status=SkillStatus.FAILED,
                             started_at=started_at, error_message=str(e))

    def _format_report(self, report: ValueJudgmentReport) -> str:
        lines = ["# ðŸ“ˆ Q3P1 Trend Value Report\n", f"**Trends Assessed:** {len(report.assessments)}\n"]
        for i, a in enumerate(report.assessments[:10], 1):
            emoji = {"critical": "ðŸ”¥", "high": "ðŸ“ˆ", "medium": "ðŸ“Š", "low": "ðŸ“‰"}.get(a.priority.value, "ðŸ“‰")
            lines.extend([f"### {emoji} #{i} {a.priority.value.upper()} - Score: {a.overall_score:.0f}",
                         f"**Timing:** {a.dimensions[ValueDimension.TIMING]:.0f}/100",
                         f"**Reasoning:** {a.reasoning}", f"**Action:** {a.recommended_action}", ""])
        return "\n".join(lines)


@register_skill
class DemandAssessmentAgent(Skill):
    """Q4P1 Agent: Evaluates potential demand conversion.

    This agent receives scenario/demand signals from Q4P0 (SceneDiscoverAgent) and
    assesses which usage scenarios have the highest potential to convert
    potential users into active customers.

    Value Assessment Dimensions:
        1. Market Readiness (30%): Are users ready for this solution?
        2. Problem-Solution Fit (25%): Does our solution match the scenario?
        3. Education Cost (20%): How much explanation is needed?
        4. Timing Window (15%): Is this the right moment?
        5. Competition Gap (10%): Is there unmet demand?

    Output:
        - Scenario viability assessments
        - Conversion probability estimates
        - Education strategy recommendations
    """
    skill_type = "demand_assessment_agent"
    name = "Demand Assessment Agent (Q4P1)"
    description = "P1 value judgment agent for demand evaluation"

    parameters_schema = {
        "signal_sources": {
            "type": "array",
            "items": {"type": "string"},
            "default": ["shared_memory"]
        },
        "min_conversion_potential": {
            "type": "integer",
            "default": 55,
            "minimum": 0,
            "maximum": 100
        },
        "hours_lookback": {
            "type": "integer",
            "default": 24,
            "minimum": 1,
            "maximum": 168
        },
        "target_notebook_id": {
            "type": "string",
            "default": ""
        }
    }

    def __init__(self, config: SkillConfig):
        self.signal_sources = config.parameters.get("signal_sources", ["shared_memory"])
        self.min_conversion_potential = config.parameters.get("min_conversion_potential", 55)
        self.hours_lookback = config.parameters.get("hours_lookback", 24)
        self.target_notebook_id = config.parameters.get("target_notebook_id", "")
        self.shared_memory = SharedMemory()
        self.assessments: List[ValueAssessment] = []
        super().__init__(config)

    async def _fetch_q4p0_signals(self) -> List[Dict[str, Any]]:
        """Fetch scenario signals from Q4P0 layer."""
        signals = []
        for source in self.signal_sources:
            if source == "shared_memory":
                recent = self.shared_memory.get_recent_signals(hours=self.hours_lookback)
                scene_signals = [
                    s for s in recent
                    if s.get("source_quadrants") and "Q4" in s.get("source_quadrants", [])
                ]
                signals.extend(scene_signals)
        logger.info(f"Fetched {len(signals)} scenario signals from Q4P0")
        return signals

    def _assess_market_readiness(self, signal: Dict[str, Any]) -> float:
        """Assess if market is ready for this solution (0-100)."""
        score = 55  # Default moderate readiness

        # Trigger events indicate readiness
        triggers = signal.get("trigger_events", [])
        if triggers:
            score += min(len(triggers) * 10, 30)

        # Target user clarity
        target_user = signal.get("target_user", "")
        if target_user and target_user != "general":
            score += 10  # Specific audience = easier to reach

        return min(score, 100)

    def _assess_problem_solution_fit(self, signal: Dict[str, Any]) -> float:
        """Assess how well our solution fits the scenario (0-100)."""
        score = 60  # Default decent fit

        awaken_need = signal.get("awaken_need", "")
        scene = signal.get("scene", "")

        # Check alignment indicators
        alignment_keywords = ["efficiency", "save time", "reduce cost", "improve"]
        if any(kw in (awaken_need + scene).lower() for kw in alignment_keywords):
            score += 15

        return min(score, 100)

    def _assess_education_cost(self, signal: Dict[str, Any]) -> float:
        """Assess how much education is needed (0-100, higher = lower cost)."""
        score = 60  # Default moderate cost

        # Complex scenarios need more education
        scene = signal.get("scene", "")
        if len(scene) > 50:  # Detailed scenario = more complex
            score -= 10

        # Clear need = less education needed
        need = signal.get("awaken_need", "")
        if any(word in need.lower() for word in ["need", "want", "looking for"]):
            score += 15

        return max(score, 0)

    def _assess_timing_window(self, signal: Dict[str, Any]) -> float:
        """Assess timing appropriateness (0-100)."""
        score = 60

        preparation = signal.get("preparation_window", "")
        if "now" in preparation.lower():
            score += 20  # Immediate opportunity
        elif "soon" in preparation.lower():
            score += 10

        # Seasonal timing
        timing = signal.get("timing", "")
        if timing:
            try:
                month = int(timing.split("-")[0])
                current_month = datetime.utcnow().month
                if abs(month - current_month) <= 1:
                    score += 15  # Current/next month
            except:
                pass

        return min(score, 100)

    def _assess_scenario(self, signal: Dict[str, Any]) -> ValueAssessment:
        """Perform full assessment of a scenario signal."""
        signal_id = signal.get("signal_id", str(hash(str(signal))))

        readiness = self._assess_market_readiness(signal)
        fit = self._assess_problem_solution_fit(signal)
        education = self._assess_education_cost(signal)
        timing = self._assess_timing_window(signal)

        # Competition gap (inverse of known alternatives)
        competition_gap = 70  # Assume some gap exists

        # Weighted overall score
        weights = {"readiness": 0.30, "fit": 0.25, "education": 0.20, "timing": 0.15, "gap": 0.10}
        overall_score = (
            readiness * weights["readiness"] +
            fit * weights["fit"] +
            education * weights["education"] +
            timing * weights["timing"] +
            competition_gap * weights["gap"]
        )

        # Determine priority
        if overall_score >= 80 and readiness >= 75:
            priority = ValuePriority.CRITICAL
        elif overall_score >= 70:
            priority = ValuePriority.HIGH
        elif overall_score >= self.min_conversion_potential:
            priority = ValuePriority.MEDIUM
        elif overall_score >= 40:
            priority = ValuePriority.LOW
        else:
            priority = ValuePriority.REJECT

        # Reasoning
        reasoning_parts = []
        if readiness >= 75:
            reasoning_parts.append("Market is ready")
        if fit >= 75:
            reasoning_parts.append("Strong problem-solution fit")
        if timing >= 75:
            reasoning_parts.append("Optimal timing window")

        reasoning = "; ".join(reasoning_parts) if reasoning_parts else "Moderate conversion potential"

        # Recommended action
        scene = signal.get("scene", "")
        if priority == ValuePriority.CRITICAL:
            action = f"Launch targeted campaign for '{scene}' immediately"
        elif priority == ValuePriority.HIGH:
            action = f"Develop scenario-specific content for '{scene}'"
        elif priority == ValuePriority.MEDIUM:
            action = f"Prepare educational materials for '{scene}'"
        elif priority == ValuePriority.LOW:
            action = f"Monitor '{scene}' for future opportunity"
        else:
            action = "Decline - insufficient conversion potential"

        return ValueAssessment(
            signal_id=signal_id,
            source_quadrant="Q4",
            overall_score=overall_score,
            priority=priority,
            dimensions={
                ValueDimension.URGENCY: timing,
                ValueDimension.IMPACT: readiness,
                ValueDimension.FEASIBILITY: education,
                ValueDimension.ALIGNMENT: fit,
                ValueDimension.TIMING: timing
            },
            reasoning=reasoning,
            recommended_action=action,
            confidence=0.70
        )

    async def execute(self, context: SkillContext) -> SkillResult:
        """Execute Q4P1 demand assessment."""
        started_at = datetime.utcnow()

        try:
            signals = await self._fetch_q4p0_signals()
            if not signals:
                return SkillResult(
                    skill_id=context.skill_id,
                    status=SkillStatus.SUCCESS,
                    started_at=started_at,
                    output={"message": "No scenario signals found", "assessments": []}
                )

            self.assessments = []
            for signal in signals:
                assessment = self._assess_scenario(signal)
                if assessment.priority != ValuePriority.REJECT:
                    self.assessments.append(assessment)

            self.assessments.sort(key=lambda x: x.overall_score, reverse=True)

            # Generate report
            report = ValueJudgmentReport(agent_name="DemandAssessmentAgent", quadrant="Q4")
            report.assessments = self.assessments
            report.prioritized_signals = [a.signal_id for a in self.assessments[:10]]

            high_count = sum(1 for a in self.assessments if a.priority == ValuePriority.HIGH)
            if high_count >= 2:
                report.insights.append(f"{high_count} high-conversion scenarios identified")

            # Store to SharedMemory
            for assessment in self.assessments:
                key = f"p1_assessment:q4:{assessment.signal_id}"
                self.shared_memory.store(key, assessment.to_dict(), ttl_seconds=86400)

            output = {
                "total_scenarios": len(signals),
                "qualified": len(self.assessments),
                "top_scenarios": [a.to_dict() for a in self.assessments[:5]],
                "report": report.to_dict()
            }

            created_note_ids = []
            if self.target_notebook_id:
                note = Note(
                    title=f"[Q4P1] Demand Assessment - {datetime.now().strftime('%Y-%m-%d')}",
                    content=self._format_report(report),
                    note_type="ai"
                )
                await note.save()
                await note.add_to_notebook(self.target_notebook_id)
                created_note_ids.append(str(note.id))

            return SkillResult(
                skill_id=context.skill_id,
                status=SkillStatus.SUCCESS,
                started_at=started_at,
                completed_at=datetime.utcnow(),
                output=output,
                created_note_ids=created_note_ids
            )

        except Exception as e:
            logger.exception(f"Q4P1 execution failed: {e}")
            return SkillResult(skill_id=context.skill_id, status=SkillStatus.FAILED,
                             started_at=started_at, error_message=str(e))

    def _format_report(self, report: ValueJudgmentReport) -> str:
        lines = ["# ðŸŽ¯ Q4P1 Demand Assessment Report\n", f"**Scenarios Assessed:** {len(report.assessments)}\n"]
        for i, a in enumerate(report.assessments[:10], 1):
            emoji = {"critical": "ðŸŽ¯", "high": "ðŸ’¡", "medium": "ðŸ”", "low": "ðŸ’­"}.get(a.priority.value, "ðŸ’­")
            lines.extend([f"### {emoji} #{i} {a.priority.value.upper()} - Score: {a.overall_score:.0f}",
                         f"**Readiness:** {a.dimensions[ValueDimension.IMPACT]:.0f}/100",
                         f"**Reasoning:** {a.reasoning}", f"**Action:** {a.recommended_action}", ""])
        return "\n".join(lines)
