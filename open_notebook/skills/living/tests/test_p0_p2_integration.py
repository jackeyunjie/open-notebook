"""P0-P2 Integration Test - æ„ŸçŸ¥å±‚åˆ°å…³ç³»å±‚é›†æˆæµ‹è¯•

æµ‹è¯•æ´»ä½“çŸ¥è¯†ç³»ç»Ÿä¸­ P0(æ„ŸçŸ¥å±‚) -> P1(åˆ¤æ–­å±‚) -> P2(å…³ç³»å±‚) çš„å®Œæ•´æ•°æ®æµ
"""

import asyncio
import hashlib
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import pytest
from loguru import logger

# P0 ç»„ä»¶
from open_notebook.skills.living.examples.p0_perception_organ import (
    create_pain_scanner_skill,
    create_emotion_watcher_skill,
    create_trend_hunter_skill,
    create_scene_discover_skill,
    create_p0_perception_agent,
)

# P1 ç»„ä»¶
from open_notebook.skills.living.p1_judgment_layer import (
    create_p1_judgment_organ,
    ValueAssessment,
    JudgmentDimension,
)

# P2 ç»„ä»¶
from open_notebook.skills.living.p2_relationship_layer import (
    create_p2_relationship_organ,
    RelationshipAnalysis,
    KnowledgeGraph,
    RelationshipType,
)


class TestP0PerceptionLayer:
    """æµ‹è¯• P0 æ„ŸçŸ¥å±‚"""

    @pytest.mark.asyncio
    async def test_pain_scanner_skill(self):
        """æµ‹è¯•ç—›ç‚¹æ‰«ææŠ€èƒ½"""
        skill = create_pain_scanner_skill()

        context = {
            "platform_data": {
                "comments": [
                    {"text": "I have a problem with login", "platform": "weibo", "timestamp": "2024-01-01"},
                    {"text": "This feature is frustrating", "platform": "xiaohongshu", "timestamp": "2024-01-01"},
                    {"text": "Love this!", "platform": "douyin", "timestamp": "2024-01-01"},
                ]
            }
        }

        result = await skill.invoke(context)

        assert result is not None
        assert "pain_points" in result
        assert result["count"] >= 2  # è‡³å°‘æ£€æµ‹åˆ°2ä¸ªç—›ç‚¹
        print(f"âœ… Pain Scanner: æ£€æµ‹åˆ° {result['count']} ä¸ªç—›ç‚¹")

    @pytest.mark.asyncio
    async def test_emotion_watcher_skill(self):
        """æµ‹è¯•æƒ…æ„Ÿç›‘æ§æŠ€èƒ½"""
        skill = create_emotion_watcher_skill()

        context = {
            "content": [
                {"text": "I'm so happy with this product!", "timestamp": "2024-01-01"},
                {"text": "This makes me angry", "timestamp": "2024-01-01"},
                {"text": "Love it!", "timestamp": "2024-01-01"},
            ]
        }

        result = await skill.invoke(context)

        assert result is not None
        assert "emotions" in result
        assert "dominant_emotion" in result
        print(f"âœ… Emotion Watcher: ä¸»å¯¼æƒ…æ„Ÿ = {result['dominant_emotion']}")

    @pytest.mark.asyncio
    async def test_trend_hunter_skill(self):
        """æµ‹è¯•è¶‹åŠ¿å‘ç°æŠ€èƒ½"""
        skill = create_trend_hunter_skill()

        context = {
            "hashtags": [
                {"name": "AI", "volume": 10000, "growth": 15.5},
                {"name": "MachineLearning", "volume": 5000, "growth": 8.2},
                {"name": "startup", "volume": 3000, "growth": 20.0},
            ]
        }

        result = await skill.invoke(context)

        assert result is not None
        assert "trends" in result
        assert len(result["trends"]) > 0
        print(f"âœ… Trend Hunter: å‘ç° {len(result['trends'])} ä¸ªè¶‹åŠ¿")

    @pytest.mark.asyncio
    async def test_scene_discover_skill(self):
        """æµ‹è¯•åœºæ™¯å‘ç°æŠ€èƒ½"""
        skill = create_scene_discover_skill()

        context = {
            "locations": [
                {"name": "Tech Conference", "audience": 5000, "engagement": 0.8, "competition": 2},
                {"name": "Online Forum", "audience": 10000, "engagement": 0.6, "competition": 5},
            ]
        }

        result = await skill.invoke(context)

        assert result is not None
        assert "scenes" in result
        assert "total_addressable_audience" in result
        print(f"âœ… Scene Discover: å‘ç° {len(result['scenes'])} ä¸ªåœºæ™¯")


class TestP1JudgmentLayer:
    """æµ‹è¯• P1 åˆ¤æ–­å±‚"""

    @pytest.mark.asyncio
    async def test_value_assessment(self):
        """æµ‹è¯•ä»·å€¼è¯„ä¼°"""
        organ = create_p1_judgment_organ()

        content = """
        æˆ‘å‘ç°äº†ä¸€ä¸ªé‡è¦çš„è¶‹åŠ¿ï¼šAI Agent æ­£åœ¨ä»å·¥å…·å‘åä½œè€…è½¬å˜ã€‚
        è¿™ä¸ªè½¬å˜æœ‰ä¸‰ä¸ªå…³é”®ä¿¡å·ï¼š
        1. å¤š Agent åä½œæ¡†æ¶çš„æˆç†Ÿ
        2. é•¿æœŸè®°å¿†å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›çš„æå‡
        3. ä»è¢«åŠ¨å“åº”åˆ°ä¸»åŠ¨å»ºè®®çš„æ¼”è¿›
        """

        metadata = {
            "id": "test_001",
            "type": "insight",
            "created_at": datetime.now().isoformat(),
            "author": "AIç ”ç©¶å‘˜",
            "tags": ["AI", "Agent", "è¶‹åŠ¿"],
            "source_type": "expert"
        }

        assessment = await organ.assess(content, metadata)

        assert assessment is not None
        assert isinstance(assessment, ValueAssessment)
        assert 0 <= assessment.overall_score <= 1
        assert assessment.priority in ["low", "normal", "high", "critical"]
        assert len(assessment.judgments) == 4  # å››ä¸ªç»´åº¦

        print(f"âœ… P1 Judgment: æ€»åˆ†={assessment.overall_score:.2f}, ä¼˜å…ˆçº§={assessment.priority}")

    @pytest.mark.asyncio
    async def test_all_dimensions(self):
        """æµ‹è¯•æ‰€æœ‰åˆ¤æ–­ç»´åº¦"""
        organ = create_p1_judgment_organ()

        content = "æµ‹è¯•å†…å®¹"
        metadata = {
            "id": "test_002",
            "type": "note",
            "created_at": datetime.now().isoformat(),
        }

        assessment = await organ.assess(content, metadata)

        dimensions = [d for d in JudgmentDimension]
        for dim in dimensions:
            assert dim in assessment.judgments
            result = assessment.judgments[dim]
            assert 0 <= result.score <= 1
            assert 0 <= result.confidence <= 1
            assert len(result.reasoning) > 0

        print(f"âœ… P1 Judgment: æ‰€æœ‰ {len(dimensions)} ä¸ªç»´åº¦è¯„ä¼°å®Œæˆ")


class TestP2RelationshipLayer:
    """æµ‹è¯• P2 å…³ç³»å±‚"""

    @pytest.mark.asyncio
    async def test_relationship_analysis(self):
        """æµ‹è¯•å…³ç³»åˆ†æ"""
        organ = create_p2_relationship_organ()

        content = """
        AI Agent æ­£åœ¨ä»å·¥å…·å‘åä½œè€…è½¬å˜

        2024å¹´ä»¥æ¥ï¼Œè¿™ä¸ªè¶‹åŠ¿è¶Šæ¥è¶Šæ˜æ˜¾ã€‚æ ¹æ® OpenAI çš„ç ”ç©¶ï¼Œ
        å¤š Agent åä½œæ¡†æ¶æ­£åœ¨å¿«é€Ÿæˆç†Ÿã€‚

        å»ºè®®ç«‹å³å…³æ³¨è¿™ä¸ªé¢†åŸŸã€‚
        https://openai.com/research
        """

        metadata = {
            "id": "test_p2_001",
            "type": "insight",
            "title": "AI Agent è¶‹åŠ¿åˆ†æ",
            "created_at": datetime.now().isoformat(),
            "tags": ["AI", "Agent", "è¶‹åŠ¿"]
        }

        existing_nodes = [
            {
                "id": "node_001",
                "title": "AI Agent åŸºç¡€æ¦‚å¿µ",
                "content": "Agent æ˜¯èƒ½å¤Ÿè‡ªä¸»è¡ŒåŠ¨çš„ AI ç³»ç»Ÿ",
                "tags": ["AI", "Agent"],
                "created_at": (datetime.now() - timedelta(days=30)).isoformat()
            },
            {
                "id": "node_002",
                "title": "2023 AI æŠ€æœ¯å›é¡¾",
                "content": "2023å¹´æ˜¯å¤§è¯­è¨€æ¨¡å‹çˆ†å‘çš„ä¸€å¹´",
                "tags": ["AI", "2023", "å›é¡¾"],
                "created_at": (datetime.now() - timedelta(days=15)).isoformat()
            }
        ]

        analysis = await organ.analyze_relationships(content, metadata, existing_nodes)

        assert analysis is not None
        assert isinstance(analysis, RelationshipAnalysis)
        assert analysis.content_id == "test_p2_001"

        print(f"âœ… P2 Relationship: å‘ç° {len(analysis.related_nodes)} ä¸ªå…³è”èŠ‚ç‚¹")

    @pytest.mark.asyncio
    async def test_knowledge_graph_construction(self):
        """æµ‹è¯•çŸ¥è¯†å›¾è°±æ„å»º"""
        organ = create_p2_relationship_organ()

        content = "æµ‹è¯•å†…å®¹"
        metadata = {
            "id": "graph_test_001",
            "type": "note",
            "title": "æµ‹è¯•ç¬”è®°"
        }

        analysis = await organ.analyze_relationships(content, metadata)
        node = organ.add_to_graph("graph_test_001", content, metadata, analysis, p1_score=0.75)

        graph = organ.get_graph()

        assert graph is not None
        assert isinstance(graph, KnowledgeGraph)
        assert len(graph.nodes) >= 1

        print(f"âœ… P2 Graph: å›¾è°±åŒ…å« {len(graph.nodes)} ä¸ªèŠ‚ç‚¹, {len(graph.edges)} æ¡è¾¹")


class TestP0ToP2Integration:
    """æµ‹è¯• P0 -> P1 -> P2 å®Œæ•´é›†æˆæµ"""

    @pytest.mark.asyncio
    async def test_perception_to_judgment_flow(self):
        """æµ‹è¯• P0 -> P1 æ•°æ®æµ"""
        # P0: æ„ŸçŸ¥æ•°æ®
        pain_scanner = create_pain_scanner_skill()
        context = {
            "platform_data": {
                "comments": [
                    {"text": "AI tools are difficult to use", "platform": "twitter", "timestamp": "2024-01-01"},
                    {"text": "Need better AI interfaces", "platform": "reddit", "timestamp": "2024-01-01"},
                ]
            }
        }
        p0_result = await pain_scanner.invoke(context)

        # å°† P0 è¾“å‡ºè½¬æ¢ä¸º P1 è¾“å…¥
        content = f"""
        ç”¨æˆ·ç—›ç‚¹åˆ†æ:
        {p0_result['count']} ä¸ªç—›ç‚¹è¢«å‘ç°
        æœ€é«˜ä¸¥é‡åº¦: {p0_result.get('top_severity', 0)}
        
        ä¸»è¦åé¦ˆ:
        """
        for point in p0_result.get('pain_points', [])[:3]:
            content += f"- {point.get('text', '')}\n"

        metadata = {
            "id": "p0_p1_flow_001",
            "type": "perception_insight",
            "created_at": datetime.now().isoformat(),
            "tags": ["ç—›ç‚¹", "ç”¨æˆ·åé¦ˆ", "AI"],
            "source_type": "social"
        }

        # P1: ä»·å€¼åˆ¤æ–­
        p1_organ = create_p1_judgment_organ()
        assessment = await p1_organ.assess(content, metadata)

        assert assessment.overall_score > 0
        print(f"âœ… P0->P1 Flow: æ„ŸçŸ¥æ•°æ®è¯„åˆ† = {assessment.overall_score:.2f}")

    @pytest.mark.asyncio
    async def test_judgment_to_relationship_flow(self):
        """æµ‹è¯• P1 -> P2 æ•°æ®æµ"""
        # P1: ç”Ÿæˆé«˜è´¨é‡æ´å¯Ÿ
        content = """
        AI Agent æ­£åœ¨ä»å·¥å…·å‘åä½œè€…è½¬å˜
        
        å…³é”®å‘ç°:
        1. å¤š Agent åä½œæ¡†æ¶æˆç†Ÿ
        2. é•¿æœŸè®°å¿†èƒ½åŠ›æå‡
        3. ä»è¢«åŠ¨åˆ°ä¸»åŠ¨å»ºè®®
        
        å»ºè®®: ç«‹å³å…³æ³¨æ­¤é¢†åŸŸ
        """

        metadata = {
            "id": "p1_p2_flow_001",
            "type": "insight",
            "title": "AI Agent è¶‹åŠ¿æ´å¯Ÿ",
            "created_at": datetime.now().isoformat(),
            "author": "ç ”ç©¶å‘˜",
            "tags": ["AI", "Agent", "è¶‹åŠ¿"],
            "source_type": "expert"
        }

        p1_organ = create_p1_judgment_organ()
        assessment = await p1_organ.assess(content, metadata)

        # åªæœ‰é€šè¿‡åˆ¤æ–­çš„å†…å®¹æ‰è¿›å…¥ P2
        if assessment.overall_score >= 0.4:
            p2_organ = create_p2_relationship_organ()

            existing_nodes = [
                {
                    "id": "existing_001",
                    "title": "Multi-Agent Systems",
                    "content": "Multi-agent collaboration frameworks",
                    "tags": ["AI", "Agent"],
                    "created_at": (datetime.now() - timedelta(days=7)).isoformat()
                }
            ]

            analysis = await p2_organ.analyze_relationships(content, metadata, existing_nodes)
            node = p2_organ.add_to_graph(
                metadata["id"],
                content,
                metadata,
                analysis,
                p1_score=assessment.overall_score
            )

            assert node is not None
            assert node.p1_score == assessment.overall_score
            print(f"âœ… P1->P2 Flow: é«˜åˆ†å†…å®¹({assessment.overall_score:.2f})è¿›å…¥çŸ¥è¯†å›¾è°±")

    @pytest.mark.asyncio
    async def test_full_pipeline(self):
        """æµ‹è¯•å®Œæ•´ P0 -> P1 -> P2 ç®¡é“"""
        print("\n" + "="*60)
        print("å¼€å§‹å®Œæ•´ç®¡é“æµ‹è¯•: P0 -> P1 -> P2")
        print("="*60)

        # ========== P0: æ„ŸçŸ¥å±‚ ==========
        print("\n[1/3] P0 æ„ŸçŸ¥å±‚ - æ”¶é›†å¸‚åœºæƒ…æŠ¥...")

        # æ¨¡æ‹Ÿå¤šä¸ªæ„ŸçŸ¥æŠ€èƒ½å¹¶è¡Œæ‰§è¡Œ
        pain_scanner = create_pain_scanner_skill()
        emotion_watcher = create_emotion_watcher_skill()
        trend_hunter = create_trend_hunter_skill()

        p0_contexts = {
            "pain": {
                "platform_data": {
                    "comments": [
                        {"text": "I have a problem with AI collaboration", "platform": "twitter", "timestamp": "2024-01-01"},
                        {"text": "This feature is frustrating and difficult to use", "platform": "reddit", "timestamp": "2024-01-01"},
                        {"text": "I'm struggling with multi-agent systems", "platform": "hackernews", "timestamp": "2024-01-01"},
                    ]
                }
            },
            "emotion": {
                "content": [
                    {"text": "Excited about AI agents!", "timestamp": "2024-01-01"},
                    {"text": "Worried about complexity", "timestamp": "2024-01-01"},
                    {"text": "Hopeful for the future", "timestamp": "2024-01-01"},
                ]
            },
            "trend": {
                "hashtags": [
                    {"name": "AIAgent", "volume": 50000, "growth": 25.5},
                    {"name": "MultiAgent", "volume": 20000, "growth": 35.0},
                    {"name": "AgentFramework", "volume": 15000, "growth": 40.2},
                ]
            }
        }

        # å¹¶è¡Œæ‰§è¡Œ P0 æŠ€èƒ½
        p0_results = await asyncio.gather(
            pain_scanner.invoke(p0_contexts["pain"]),
            emotion_watcher.invoke(p0_contexts["emotion"]),
            trend_hunter.invoke(p0_contexts["trend"]),
        )

        pain_result, emotion_result, trend_result = p0_results

        print(f"  âœ… Pain Scanner: {pain_result['count']} ç—›ç‚¹")
        print(f"  âœ… Emotion Watcher: ä¸»å¯¼æƒ…æ„Ÿ={emotion_result['dominant_emotion']}")
        print(f"  âœ… Trend Hunter: {len(trend_result['trends'])} è¶‹åŠ¿")

        # æ•´åˆ P0 è¾“å‡ºä¸ºç»“æ„åŒ–å†…å®¹
        integrated_content = f"""
# å¸‚åœºæƒ…æŠ¥ç»¼åˆåˆ†æ

## ç”¨æˆ·ç—›ç‚¹ ({pain_result['count']}ä¸ª)
"""
        for point in pain_result.get('pain_points', [])[:3]:
            integrated_content += f"- [{point.get('source', '')}] {point.get('text', '')}\n"

        integrated_content += f"""
## æƒ…æ„Ÿè¶‹åŠ¿
- ä¸»å¯¼æƒ…æ„Ÿ: {emotion_result['dominant_emotion']}
- æƒ…æ„Ÿåˆ†å¸ƒ: {emotion_result['emotions']}

## çƒ­é—¨è¶‹åŠ¿
"""
        for trend in trend_result.get('trends', [])[:3]:
            integrated_content += f"- #{trend['name']}: å¢é•¿ç‡ {trend['growth_rate']}%\n"

        # ========== P1: åˆ¤æ–­å±‚ ==========
        print("\n[2/3] P1 åˆ¤æ–­å±‚ - è¯„ä¼°å†…å®¹ä»·å€¼...")

        p1_metadata = {
            "id": f"integrated_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "type": "market_intelligence",
            "title": "AI Agent å¸‚åœºæƒ…æŠ¥",
            "created_at": datetime.now().isoformat(),
            "tags": ["AI", "Agent", "å¸‚åœºæƒ…æŠ¥", "ç”¨æˆ·ç—›ç‚¹", "è¶‹åŠ¿"],
            "source_type": "social",
            "focus_areas": ["AI", "Agent", "å¸‚åœºè¶‹åŠ¿"]
        }

        p1_organ = create_p1_judgment_organ()
        assessment = await p1_organ.assess(integrated_content, p1_metadata)

        print(f"  âœ… ç»¼åˆè¯„åˆ†: {assessment.overall_score:.2f}")
        print(f"  âœ… ä¼˜å…ˆçº§: {assessment.priority}")
        print(f"  âœ… å»ºè®®: {assessment.recommended_action}")

        for dim, result in assessment.judgments.items():
            print(f"    - {dim.value}: {result.score:.2f}")

        # ========== P2: å…³ç³»å±‚ ==========
        print("\n[3/3] P2 å…³ç³»å±‚ - æ„å»ºçŸ¥è¯†å…³è”...")

        p2_organ = create_p2_relationship_organ()

        # æ¨¡æ‹Ÿå·²æœ‰çŸ¥è¯†èŠ‚ç‚¹
        existing_nodes = [
            {
                "id": "knowledge_001",
                "title": "AI Agent åŸºç¡€æ¦‚å¿µ",
                "content": "Agent æ˜¯èƒ½å¤Ÿè‡ªä¸»è¡ŒåŠ¨çš„ AI ç³»ç»Ÿï¼Œå…·æœ‰æ„ŸçŸ¥ã€å†³ç­–å’Œæ‰§è¡Œèƒ½åŠ›",
                "tags": ["AI", "Agent", "åŸºç¡€"],
                "created_at": (datetime.now() - timedelta(days=30)).isoformat()
            },
            {
                "id": "knowledge_002",
                "title": "Multi-Agent Systems ç»¼è¿°",
                "content": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œæœºåˆ¶å’Œé€šä¿¡åè®®",
                "tags": ["AI", "Agent", "Multi-Agent"],
                "created_at": (datetime.now() - timedelta(days=15)).isoformat()
            },
            {
                "id": "knowledge_003",
                "title": "2024 AI è¶‹åŠ¿é¢„æµ‹",
                "content": "2024å¹´ AI å‘å±•çš„ä¸»è¦æ–¹å‘å’Œè¶‹åŠ¿",
                "tags": ["AI", "è¶‹åŠ¿", "2024"],
                "created_at": (datetime.now() - timedelta(days=7)).isoformat()
            }
        ]

        analysis = await p2_organ.analyze_relationships(
            integrated_content,
            p1_metadata,
            existing_nodes
        )

        # å°†å†…å®¹åŠ å…¥çŸ¥è¯†å›¾è°±
        node = p2_organ.add_to_graph(
            p1_metadata["id"],
            integrated_content,
            p1_metadata,
            analysis,
            p1_score=assessment.overall_score
        )

        # è·å–å­å›¾
        graph = p2_organ.get_graph()

        print(f"  âœ… å‘ç° {len(analysis.related_nodes)} ä¸ªå…³è”èŠ‚ç‚¹")
        print(f"  âœ… å»ºè®® {len(analysis.suggested_connections)} ä¸ªè¿æ¥")
        print(f"  âœ… å›¾è°±æ´å¯Ÿ:")
        for insight in analysis.graph_insights[:3]:
            print(f"    - {insight}")

        print(f"\n  ğŸ“Š çŸ¥è¯†å›¾è°±ç»Ÿè®¡:")
        print(f"    - èŠ‚ç‚¹æ•°: {len(graph.nodes)}")
        print(f"    - è¾¹æ•°: {len(graph.edges)}")

        # ========== éªŒè¯ ==========
        print("\n" + "="*60)
        print("ç®¡é“éªŒè¯")
        print("="*60)

        # éªŒè¯æ•°æ®æµå®Œæ•´æ€§
        assert pain_result["count"] > 0, "P0 åº”è¯¥æ”¶é›†åˆ°ç—›ç‚¹æ•°æ®"
        assert assessment.overall_score > 0, "P1 åº”è¯¥äº§ç”Ÿæœ‰æ•ˆè¯„åˆ†"
        assert node.p1_score == assessment.overall_score, "P2 åº”è¯¥ç»§æ‰¿ P1 è¯„åˆ†"
        assert len(graph.nodes) > 0, "P2 åº”è¯¥æ„å»ºçŸ¥è¯†å›¾è°±"

        print("âœ… å®Œæ•´ç®¡é“æµ‹è¯•é€šè¿‡!")
        print(f"   æ•°æ®æµ: P0({pain_result['count']}ç—›ç‚¹) -> P1({assessment.overall_score:.2f}åˆ†) -> P2({len(graph.nodes)}èŠ‚ç‚¹)")

        return {
            "p0": {
                "pain_count": pain_result["count"],
                "dominant_emotion": emotion_result["dominant_emotion"],
                "trend_count": len(trend_result["trends"])
            },
            "p1": {
                "score": assessment.overall_score,
                "priority": assessment.priority,
                "dimensions": {dim.value: result.score for dim, result in assessment.judgments.items()}
            },
            "p2": {
                "related_nodes": len(analysis.related_nodes),
                "suggestions": len(analysis.suggested_connections),
                "graph_nodes": len(graph.nodes),
                "graph_edges": len(graph.edges)
            }
        }


class TestEdgeCases:
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""

    @pytest.mark.asyncio
    async def test_low_quality_content_filtering(self):
        """æµ‹è¯•ä½è´¨é‡å†…å®¹è¿‡æ»¤"""
        p1_organ = create_p1_judgment_organ()

        # ä½è´¨é‡å†…å®¹
        low_quality_content = "ok"
        metadata = {
            "id": "low_quality_001",
            "type": "note",
            "created_at": datetime.now().isoformat(),
        }

        assessment = await p1_organ.assess(low_quality_content, metadata)

        # ä½è´¨é‡å†…å®¹åº”è¯¥å¾—åˆ°ä½åˆ†
        assert assessment.overall_score < 0.5
        assert assessment.priority in ["low", "normal"]

        print(f"âœ… ä½è´¨é‡å†…å®¹è¿‡æ»¤: è¯„åˆ†={assessment.overall_score:.2f}, ä¼˜å…ˆçº§={assessment.priority}")

    @pytest.mark.asyncio
    async def test_empty_content_handling(self):
        """æµ‹è¯•ç©ºå†…å®¹å¤„ç†"""
        p1_organ = create_p1_judgment_organ()
        p2_organ = create_p2_relationship_organ()

        # ç©ºå†…å®¹
        empty_content = ""
        metadata = {
            "id": "empty_001",
            "type": "note",
            "title": "Empty Note"
        }

        # P1 åº”è¯¥èƒ½å¤„ç†ç©ºå†…å®¹
        assessment = await p1_organ.assess(empty_content, metadata)
        assert assessment is not None

        # P2 åº”è¯¥èƒ½å¤„ç†ç©ºå†…å®¹
        analysis = await p2_organ.analyze_relationships(empty_content, metadata)
        assert analysis is not None

        print(f"âœ… ç©ºå†…å®¹å¤„ç†: P1è¯„åˆ†={assessment.overall_score:.2f}, P2å…³è”={len(analysis.related_nodes)}")


async def run_integration_demo():
    """è¿è¡Œé›†æˆæ¼”ç¤º"""
    print("\n" + "="*70)
    print("æ´»ä½“çŸ¥è¯†ç³»ç»Ÿ - P0-P2 é›†æˆæ¼”ç¤º")
    print("="*70)

    integration_test = TestP0ToP2Integration()
    result = await integration_test.test_full_pipeline()

    print("\n" + "="*70)
    print("æ¼”ç¤ºå®Œæˆ!")
    print("="*70)
    print(f"\nç»“æœæ‘˜è¦:")
    print(f"  P0 æ„ŸçŸ¥: {result['p0']}")
    print(f"  P1 åˆ¤æ–­: {result['p1']}")
    print(f"  P2 å…³ç³»: {result['p2']}")

    return result


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(run_integration_demo())
