"""
Cross-Document Insights - è·¨æ–‡æ¡£æ´å¯Ÿç³»ç»Ÿ

åŠŸèƒ½:
1. å‘ç°å¤šä¸ª Source ä¹‹é—´çš„å…±æ€§å’ŒçŸ›ç›¾
2. è¯†åˆ«ä¸»é¢˜æ¼”åŒ–è¶‹åŠ¿
3. è‡ªåŠ¨ç”Ÿæˆ"æœ¬å‘¨ç ”ç©¶è¶‹åŠ¿"æŠ¥å‘Š
4. æ¦‚å¿µèšç±»å’Œå…³è”åˆ†æ
5. çŸ›ç›¾è§‚ç‚¹æ£€æµ‹
"""

import asyncio
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple

from loguru import logger

from open_notebook.domain.notebook import Notebook, Source


class CrossDocumentAnalyzer:
    """è·¨æ–‡æ¡£åˆ†æå™¨"""
    
    def __init__(self, notebook_id: str):
        self.notebook_id = notebook_id
        self.notebook: Optional[Notebook] = None
        
    async def initialize(self):
        """åˆå§‹åŒ–"""
        logger.info(f"Initializing CrossDocumentAnalyzer for notebook {self.notebook_id}")
        self.notebook = await Notebook.get(self.notebook_id)
        if not self.notebook:
            raise ValueError(f"Notebook {self.notebook_id} not found")
    
    async def analyze_common_themes(
        self,
        source_ids: Optional[List[str]] = None,
        min_frequency: int = 2
    ) -> Dict[str, Any]:
        """åˆ†æå…±åŒä¸»é¢˜
        
        Args:
            source_ids: æŒ‡å®šæºåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œé»˜è®¤åˆ†ææ‰€æœ‰æºï¼‰
            min_frequency: æœ€å°å‡ºç°é¢‘æ¬¡
            
        Returns:
            å…±åŒä¸»é¢˜åˆ†æç»“æœ
        """
        sources = await self._get_sources(source_ids)
        
        # ç»Ÿè®¡æ‰€æœ‰ä¸»é¢˜
        all_topics = []
        topic_sources = defaultdict(list)
        
        for source in sources:
            topics = getattr(source, 'topics', []) or []
            for topic in topics:
                all_topics.append(topic)
                topic_sources[topic].append({
                    'source_id': source.id,
                    'source_title': getattr(source, 'title', 'Untitled')
                })
        
        # è®¡ç®—é¢‘ç‡
        topic_counts = Counter(all_topics)
        
        # ç­›é€‰é«˜é¢‘ä¸»é¢˜
        common_themes = {
            topic: {
                'count': count,
                'percentage': round(count / len(sources) * 100, 2) if sources else 0,
                'sources': topic_sources[topic],
                'trend': 'stable'  # TODO: è®¡ç®—è¶‹åŠ¿
            }
            for topic, count in topic_counts.items()
            if count >= min_frequency
        }
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_themes = dict(sorted(
            common_themes.items(),
            key=lambda x: x[1]['count'],
            reverse=True
        ))
        
        return {
            'total_sources': len(sources),
            'total_topics': len(all_topics),
            'unique_topics': len(topic_counts),
            'common_themes': sorted_themes,
            'top_themes': list(sorted_themes.items())[:10]
        }
    
    async def detect_contradictions(
        self,
        source_ids: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """æ£€æµ‹çŸ›ç›¾è§‚ç‚¹
        
        Args:
            source_ids: æŒ‡å®šæºåˆ—è¡¨
            
        Returns:
            çŸ›ç›¾è§‚ç‚¹åˆ—è¡¨
        """
        sources = await self._get_sources(source_ids)
        contradictions = []
        
        # TODO: å®ç°æ™ºèƒ½çŸ›ç›¾æ£€æµ‹
        # å½“å‰ç‰ˆæœ¬ä½¿ç”¨ç®€åŒ–ç­–ç•¥ï¼šæ£€æµ‹ç›¸åŒä¸»é¢˜ä¸‹çš„ä¸åŒè§‚ç‚¹
        
        # æŒ‰ä¸»é¢˜åˆ†ç»„
        topic_groups = defaultdict(list)
        for source in sources:
            topics = getattr(source, 'topics', []) or []
            for topic in topics:
                topic_groups[topic].append(source)
        
        # æ£€æµ‹æ¯ä¸ªä¸»é¢˜ä¸‹çš„æ½œåœ¨çŸ›ç›¾
        for topic, topic_sources in topic_groups.items():
            if len(topic_sources) >= 2:
                # ç®€å•è§„åˆ™ï¼šå¦‚æœå¤šä¸ªæ–‡æ¡£è®¨è®ºåŒä¸€ä¸»é¢˜ï¼Œå¯èƒ½å­˜åœ¨ä¸åŒè§‚ç‚¹
                contradictions.append({
                    'type': 'potential_disagreement',
                    'topic': topic,
                    'sources': [
                        {'id': s.id, 'title': getattr(s, 'title', 'Untitled')}
                        for s in topic_sources
                    ],
                    'confidence': 'low',
                    'description': f"{len(topic_sources)} ä¸ªæ–‡æ¡£è®¨è®ºäº†'{topic}'ä¸»é¢˜ï¼Œå¯èƒ½å­˜åœ¨ä¸åŒè§‚ç‚¹",
                    'action_required': 'manual_review'
                })
        
        return contradictions
    
    async def identify_trends(
        self,
        days: int = 7,
        top_n: int = 10
    ) -> Dict[str, Any]:
        """è¯†åˆ«ç ”ç©¶è¶‹åŠ¿
        
        Args:
            days: åˆ†ææœ€è¿‘ N å¤©
            top_n: è¿”å›å‰ N ä¸ªè¶‹åŠ¿
            
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        cutoff_date = datetime.now() - timedelta(days=days)
        
        # è·å–æœ€è¿‘çš„æº
        recent_sources = []
        all_sources = await self.notebook.get_sources()
        
        for source in all_sources:
            # TODO: æ£€æŸ¥ source çš„åˆ›å»ºæ—¶é—´
            recent_sources.append(source)
        
        # åˆ†æä¸»é¢˜åˆ†å¸ƒ
        recent_topics = []
        for source in recent_sources:
            topics = getattr(source, 'topics', []) or []
            recent_topics.extend(topics)
        
        topic_counts = Counter(recent_topics)
        
        # è¯†åˆ«ä¸Šå‡æœ€å¿«çš„ä¸»é¢˜
        trending_topics = []
        for topic, count in topic_counts.most_common(top_n):
            trending_topics.append({
                'topic': topic,
                'count': count,
                'percentage': round(count / len(recent_topics) * 100, 2) if recent_topics else 0,
                'velocity': 'high'  # TODO: è®¡ç®—å¢é•¿é€Ÿåº¦
            })
        
        return {
            'period_days': days,
            'total_sources': len(recent_sources),
            'total_topics': len(recent_topics),
            'trending_topics': trending_topics,
            'hot_topics': trending_topics[:3],
            'emerging_topics': self._identify_emerging_topics(topic_counts, days)
        }
    
    def _identify_emerging_topics(
        self,
        topic_counts: Counter,
        days: int
    ) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ–°å…´ä¸»é¢˜"""
        # TODO: éœ€è¦å†å²æ•°æ®å¯¹æ¯”
        # å½“å‰ç‰ˆæœ¬ç®€åŒ–å®ç°ï¼šå‡è®¾ä½é¢‘ä½†æ–°å‡ºç°çš„ä¸»é¢˜æ˜¯æ–°å…´ä¸»é¢˜
        
        emerging = []
        total_count = sum(topic_counts.values())
        
        for topic, count in topic_counts.items():
            percentage = count / total_count if total_count > 0 else 0
            if 0 < percentage < 5:  # å æ¯”å°äº 5% å¯èƒ½æ˜¯æ–°å…´ä¸»é¢˜
                emerging.append({
                    'topic': topic,
                    'count': count,
                    'percentage': round(percentage, 2),
                    'potential': 'medium'
                })
        
        return emerging[:5]
    
    async def cluster_concepts(
        self,
        source_ids: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """èšç±»æ¦‚å¿µ
        
        Args:
            source_ids: æŒ‡å®šæºåˆ—è¡¨
            
        Returns:
            æ¦‚å¿µèšç±»ç»“æœ
        """
        sources = await self._get_sources(source_ids)
        
        # æå–æ‰€æœ‰æ¦‚å¿µ
        all_concepts = []
        concept_cooccurrence = defaultdict(lambda: defaultdict(int))
        
        for source in sources:
            concepts = getattr(source, 'topics', []) or []
            all_concepts.extend(concepts)
            
            # è®°å½•å…±ç°å…³ç³»
            for i, c1 in enumerate(concepts):
                for c2 in concepts[i+1:]:
                    concept_cooccurrence[c1][c2] += 1
                    concept_cooccurrence[c2][c1] += 1
        
        # ç®€å•çš„åŸºäºé¢‘ç‡çš„èšç±»
        concept_counts = Counter(all_concepts)
        
        # æ‰¾å‡ºå¼ºå…³è”çš„æ¦‚å¿µå¯¹
        strong_links = []
        for c1, related in concept_cooccurrence.items():
            for c2, count in related.items():
                if count >= 2 and c1 < c2:  # é¿å…é‡å¤
                    strong_links.append((c1, c2, count))
        
        # ç”Ÿæˆèšç±»ï¼ˆç®€åŒ–ç‰ˆï¼‰
        clusters = self._generate_clusters(strong_links, concept_counts)
        
        return {
            'total_concepts': len(all_concepts),
            'unique_concepts': len(concept_counts),
            'top_concepts': concept_counts.most_common(10),
            'strong_links': strong_links[:20],
            'clusters': clusters
        }
    
    def _generate_clusters(
        self,
        links: List[Tuple[str, str, int]],
        concept_counts: Counter
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¦‚å¿µç°‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ä½¿ç”¨å¹¶æŸ¥é›†æˆ–è¿é€šåˆ†é‡ç®—æ³•ä¼šæ›´å¥½
        # å½“å‰ç‰ˆæœ¬ä½¿ç”¨ç®€å•å¯å‘å¼æ–¹æ³•
        
        clusters = []
        processed = set()
        
        # æ‰¾åˆ°æœ€æ ¸å¿ƒçš„æ¦‚å¿µï¼ˆå‡ºç°é¢‘ç‡æœ€é«˜ï¼‰
        core_concepts = [c for c, _ in concept_counts.most_common(5)]
        
        for core in core_concepts:
            if core in processed:
                continue
            
            # æ‰¾åˆ°ä¸æ ¸å¿ƒæ¦‚å¿µç›¸å…³çš„æ‰€æœ‰æ¦‚å¿µ
            cluster_members = {core}
            for c1, c2, _ in links:
                if c1 == core:
                    cluster_members.add(c2)
                elif c2 == core:
                    cluster_members.add(c1)
            
            clusters.append({
                'cluster_id': len(clusters) + 1,
                'core_concept': core,
                'members': list(cluster_members),
                'size': len(cluster_members)
            })
            
            processed.update(cluster_members)
        
        return clusters
    
    async def generate_weekly_trends_report(
        self,
        source_ids: Optional[List[str]] = None
    ) -> str:
        """ç”Ÿæˆå‘¨åº¦è¶‹åŠ¿æŠ¥å‘Š
        
        Args:
            source_ids: æŒ‡å®šæºåˆ—è¡¨
            
        Returns:
            Markdown æ ¼å¼çš„æŠ¥å‘Š
        """
        # æ‰§è¡Œå¤šé¡¹åˆ†æ
        themes_result = await self.analyze_common_themes(source_ids)
        trends_result = await self.identify_trends(days=7)
        contradictions = await self.detect_contradictions(source_ids)
        clusters_result = await self.cluster_concepts(source_ids)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# ğŸ“ˆ å‘¨åº¦ç ”ç©¶è¶‹åŠ¿æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}  
**åˆ†æå‘¨æœŸ**: æœ€è¿‘ 7 å¤©  
**è¦†ç›–æ–‡çŒ®**: {themes_result['total_sources']} ç¯‡

---

## ğŸ”¥ çƒ­é—¨ä¸»é¢˜ TOP 10

"""
        
        for i, (topic, data) in enumerate(themes_result['top_themes'], 1):
            stars = "â­" * min(5, int(data['percentage'] / 10) + 1)
            report += f"{i}. **{topic}** {stars} ({data['count']}æ¬¡æåŠï¼Œ{data['percentage']}%)\n\n"
        
        report += f"""
---

## ğŸ“Š è¶‹åŠ¿åˆ†æ

### ä¸Šå‡æœ€å¿«çš„ä¸»é¢˜
"""
        
        for item in trends_result['trending_topics'][:3]:
            report += f"- **{item['topic']}** - {item['count']}æ¬¡æåŠ ({item['percentage']}%)\n"
        
        report += f"""
### æ–°å…´ä¸»é¢˜ï¼ˆæ½œåŠ›è‚¡ï¼‰
"""
        
        for item in trends_result['emerging_topics'][:3]:
            report += f"- **{item['topic']}** - é¦–æ¬¡å‡ºç°ï¼Œå æ¯”{item['percentage']}%\n"
        
        report += f"""
---

## âš ï¸ æ½œåœ¨çŸ›ç›¾ä¸åˆ†æ­§

æ£€æµ‹åˆ° {len(contradictions)} å¤„æ½œåœ¨çŸ›ç›¾ï¼š

"""
        
        for i, contradiction in enumerate(contradictions[:3], 1):
            report += f"""### {i}. {contradiction['topic']}

**ç±»å‹**: {contradiction['type']}  
**ç½®ä¿¡åº¦**: {contradiction['confidence']}  
**æ¶‰åŠæ–‡çŒ®**: {len(contradiction['sources'])} ç¯‡  
**æè¿°**: {contradiction['description']}

**å»ºè®®è¡ŒåŠ¨**: {contradiction['action_required']}

---

"""
        
        report += f"""
## ğŸ—ºï¸ æ¦‚å¿µèšç±»

è¯†åˆ«å‡º {len(clusters_result['clusters'])} ä¸ªæ¦‚å¿µç°‡ï¼š

"""
        
        for i, cluster in enumerate(clusters_result['clusters'][:3], 1):
            members_str = ', '.join(cluster['members'][:5])
            report += f"""### ç°‡{i}: {cluster['core_concept']}

**æ ¸å¿ƒæ¦‚å¿µ**: {cluster['core_concept']}  
**æˆå‘˜æ•°é‡**: {cluster['size']} ä¸ª  
**ä¸»è¦æˆå‘˜**: {members_str}

---

"""
        
        report += f"""
## ğŸ’¡ å…³é”®æ´å¯Ÿ

### æ´å¯Ÿ 1: [å¾…è¡¥å……]
åŸºäºæ•°æ®åˆ†æï¼Œå‘ç°...

### æ´å¯Ÿ 2: [å¾…è¡¥å……]
å€¼å¾—æ³¨æ„çš„æ¨¡å¼æ˜¯...

### æ´å¯Ÿ 3: [å¾…è¡¥å……]
é¢„æµ‹æœªæ¥è¶‹åŠ¿...

---

## ğŸ“… ä¸‹å‘¨å±•æœ›

**å¯èƒ½æˆä¸ºçƒ­ç‚¹çš„ä¸»é¢˜**:
1. [é¢„æµ‹ 1]
2. [é¢„æµ‹ 2]
3. [é¢„æµ‹ 3]

**å»ºè®®å…³æ³¨çš„æ–¹å‘**:
- âœ… [æ–¹å‘ 1]
- âš ï¸ [æ–¹å‘ 2]
- âŒ [æ–¹å‘ 3]

---

*æœ¬æŠ¥å‘Šç”± Open Notebook è·¨æ–‡æ¡£åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        return report
    
    async def _get_sources(self, source_ids: Optional[List[str]] = None) -> List[Source]:
        """è·å–æºåˆ—è¡¨"""
        if source_ids:
            sources = []
            for sid in source_ids:
                source = await Source.get(sid)
                if source:
                    sources.append(source)
            return sources
        else:
            return await self.notebook.get_sources()
    
    async def close(self):
        """å…³é—­"""
        logger.info("Closing CrossDocumentAnalyzer")


# ============================================================================
# Convenience Functions
# ============================================================================

async def analyze_cross_document_themes(notebook_id: str, source_ids: Optional[List[str]] = None) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ†æè·¨æ–‡æ¡£ä¸»é¢˜"""
    analyzer = CrossDocumentAnalyzer(notebook_id)
    await analyzer.initialize()
    try:
        return await analyzer.analyze_common_themes(source_ids)
    finally:
        await analyzer.close()


async def detect_contradictions(notebook_id: str, source_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    """ä¾¿æ·å‡½æ•°ï¼šæ£€æµ‹çŸ›ç›¾è§‚ç‚¹"""
    analyzer = CrossDocumentAnalyzer(notebook_id)
    await analyzer.initialize()
    try:
        return await analyzer.detect_contradictions(source_ids)
    finally:
        await analyzer.close()


async def identify_research_trends(notebook_id: str, days: int = 7) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šè¯†åˆ«ç ”ç©¶è¶‹åŠ¿"""
    analyzer = CrossDocumentAnalyzer(notebook_id)
    await analyzer.initialize()
    try:
        return await analyzer.identify_trends(days)
    finally:
        await analyzer.close()


async def generate_weekly_trends_report(notebook_id: str, source_ids: Optional[List[str]] = None) -> str:
    """ä¾¿æ·å‡½æ•°ï¼šç”Ÿæˆå‘¨åº¦è¶‹åŠ¿æŠ¥å‘Š"""
    analyzer = CrossDocumentAnalyzer(notebook_id)
    await analyzer.initialize()
    try:
        return await analyzer.generate_weekly_trends_report(source_ids)
    finally:
        await analyzer.close()
