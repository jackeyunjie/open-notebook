# DataAgent - 数据生命周期管理

## 核心理念

DataAgent 是活体知识系统的"免疫系统"，负责：
- **数据代谢**：生成、流转、消亡的全生命周期
- **质量监控**：确保数据健康、一致、可用
- **容量管理**：防止数据膨胀、优化存储
- **血缘追踪**：数据从哪来、到哪去

---

## DataAgent 架构

```
┌─────────────────────────────────────────────────────────────┐
│                      DataAgent 核心                          │
├─────────────────────────────────────────────────────────────┤
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐    │
│  │ 生成管理 │  │ 流转监控 │  │ 质量治理 │  │ 归档清理 │    │
│  │ Generator│  │  Flow    │  │ Quality  │  │ Archive  │    │
│  └──────────┘  └──────────┘  └──────────┘  └──────────┘    │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│                    数据生命周期状态机                        │
│                                                              │
│   生成 → 活跃 → 冷却 → 归档 → 销毁                           │
│    ↑      ↓      ↓      ↓      ↓                            │
│   更新   查询   少访问  备份   删除                           │
└─────────────────────────────────────────────────────────────┘
```

---

## 1. 生成管理 (Generator)

### 职责
管理所有数据的产生源头

```python
class DataGenerator:
    """数据生成管理器"""
    
    # 数据生成登记
    generators = {
        "p0_scanner": {
            "type": "sensor",           # 传感器型：持续产生
            "rate": "1/min",            # 产生频率
            "retention": "7d",          # 活跃期保留
            "schema": "pain_signal",    # 数据格式
        },
        "p1_evaluator": {
            "type": "processor",        # 处理型：输入产生输出
            "input": ["p0_scanner"],    # 依赖输入
            "retention": "30d",
            "schema": "value_judgment",
        },
        "user_action": {
            "type": "event",            # 事件型：离散产生
            "retention": "90d",
            "schema": "interaction",
        }
    }
    
    async def register_generation(self, source, data_meta):
        """登记数据生成"""
        # 1. 验证数据格式
        await self._validate_schema(data_meta)
        
        # 2. 分配生命周期策略
        lifecycle = self._assign_lifecycle(source)
        
        # 3. 记录血缘
        await self._record_lineage(source, data_meta)
        
        # 4. 设置监控
        await self._setup_monitoring(source, lifecycle)
        
        return lifecycle
```

### 数据分级策略

| 数据类型 | 产生频率 | 活跃期 | 冷却期 | 归档期 | 销毁 |
|----------|----------|--------|--------|--------|------|
| **感知数据** | 高 | 7天 | 30天 | 90天 | 1年 |
| **判断结果** | 中 | 30天 | 90天 | 1年 | 3年 |
| **关系记录** | 中 | 90天 | 1年 | 3年 | 5年 |
| **进化策略** | 低 | 永久 | - | - | - |
| **用户交互** | 中 | 90天 | 1年 | 3年 | 5年 |

---

## 2. 流转监控 (Flow)

### 职责
监控数据在经络中的流动

```python
class DataFlowMonitor:
    """数据流转监控器"""
    
    async def monitor_meridian(self, meridian_name):
        """监控经络流量"""
        metrics = {
            "throughput": 0,      # 吞吐量 (packets/sec)
            "latency": 0,         # 延迟 (ms)
            "error_rate": 0,      # 错误率
            "backpressure": 0,    # 背压 (queue深度)
        }
        
        while True:
            # 采集指标
            stats = await self._collect_metrics(meridian_name)
            
            # 异常检测
            if stats.backpressure > 1000:
                await self._alert("经络拥堵", meridian_name)
                await self._scale_up(meridian_name)
            
            if stats.error_rate > 0.01:
                await self._alert("数据损坏", meridian_name)
                await self._quarantine(meridian_name)
            
            # 记录血缘
            await self._track_flow(meridian_name, stats)
            
            await asyncio.sleep(1)
```

### 数据血缘图

```
原始数据 → P0感知 → P1判断 → P2关系 → P3进化
    ↓        ↓        ↓        ↓        ↓
  数据源   信号ID   评估ID   行动ID   策略ID
    ↓        ↓        ↓        ↓        ↓
  元数据   依赖P0   依赖P1   依赖P2   依赖P3
```

---

## 3. 质量治理 (Quality)

### 职责
确保数据健康、一致、可信

```python
class DataQualityAgent:
    """数据质量治理器"""
    
    # 质量规则
    rules = {
        "completeness": {
            "check": "required_fields_present",
            "threshold": 0.99,      # 99%完整
        },
        "consistency": {
            "check": "cross_reference_valid",
            "threshold": 0.95,      # 95%一致
        },
        "timeliness": {
            "check": "freshness",
            "max_age": "1h",        # 1小时内
        },
        "accuracy": {
            "check": "validation_rules",
            "threshold": 0.90,      # 90%准确
        }
    }
    
    async def health_check(self, dataset):
        """健康检查"""
        report = {
            "overall_score": 0,
            "checks": {},
            "issues": [],
        }
        
        for rule_name, rule in self.rules.items():
            score = await self._run_check(dataset, rule)
            report["checks"][rule_name] = score
            
            if score < rule["threshold"]:
                report["issues"].append({
                    "rule": rule_name,
                    "score": score,
                    "threshold": rule["threshold"],
                })
        
        report["overall_score"] = sum(report["checks"].values()) / len(report["checks"])
        
        # 自动修复
        if report["issues"]:
            await self._auto_repair(dataset, report["issues"])
        
        return report
    
    async def _auto_repair(self, dataset, issues):
        """自动修复"""
        for issue in issues:
            if issue["rule"] == "completeness":
                await self._fill_missing(dataset)
            elif issue["rule"] == "consistency":
                await self._reconcile_conflict(dataset)
            elif issue["rule"] == "timeliness":
                await self._refresh_stale(dataset)
```

### 数据健康度看板

```
┌─────────────────────────────────────────┐
│         数据健康度总览                   │
├─────────────────────────────────────────┤
│  整体健康度: 87%  [████████░░]          │
├─────────────────────────────────────────┤
│  完整性: 99% ✅                          │
│  一致性: 92% ⚠️  (3处冲突待修复)         │
│  时效性: 98% ✅                          │
│  准确性: 85% ⚠️  (P2关系数据偏差)        │
├─────────────────────────────────────────┤
│  活跃数据: 1.2M 条                       │
│  冷却数据: 8.5M 条                       │
│  归档数据: 45M 条                        │
└─────────────────────────────────────────┘
```

---

## 4. 归档清理 (Archive)

### 职责
管理数据的生命周期终点

```python
class DataArchiveManager:
    """数据归档管理器"""
    
    # 生命周期策略
    lifecycle_policies = {
        "hot": {           # 热数据：SSD，高频访问
            "storage": "ssd",
            "retention": "7d",
            "compression": None,
        },
        "warm": {          # 温数据：SSD，偶尔访问
            "storage": "ssd",
            "retention": "30d",
            "compression": "lz4",
        },
        "cold": {          # 冷数据：HDD，很少访问
            "storage": "hdd",
            "retention": "1y",
            "compression": "zstd",
        },
        "frozen": {        # 冻结：对象存储，合规保留
            "storage": "s3",
            "retention": "7y",
            "compression": "zstd",
        },
    }
    
    async def lifecycle_transition(self):
        """生命周期状态转换"""
        # 每天凌晨2点执行
        
        # 1. 热数据 → 温数据 (7天未访问)
        hot_to_warm = await self._find_stale_data("hot", days=7)
        for data in hot_to_warm:
            await self._compress(data, "lz4")
            await self._update_tier(data, "warm")
        
        # 2. 温数据 → 冷数据 (30天未访问)
        warm_to_cold = await self._find_stale_data("warm", days=30)
        for data in warm_to_cold:
            await self._migrate(data, to_storage="hdd")
            await self._compress(data, "zstd")
            await self._update_tier(data, "cold")
        
        # 3. 冷数据 → 冻结 (1年未访问)
        cold_to_frozen = await self._find_stale_data("cold", days=365)
        for data in cold_to_frozen:
            await self._migrate(data, to_storage="s3")
            await self._update_tier(data, "frozen")
        
        # 4. 销毁过期数据 (超过保留期)
        to_destroy = await self._find_expired_data()
        for data in to_destroy:
            await self._secure_delete(data)
            await self._record_deletion(data)
```

### 存储成本优化

| 层级 | 存储介质 | 成本/GB/月 | 占比 | 策略 |
|------|----------|------------|------|------|
| 热数据 | NVMe SSD | ¥3 | 5% | 只保留活跃数据 |
| 温数据 | SATA SSD | ¥1 | 15% | 7天后自动降级 |
| 冷数据 | HDD | ¥0.3 | 30% | 压缩后存储 |
| 冻结 | 对象存储 | ¥0.12 | 50% | 长期归档 |

**成本对比：**
- 无分层：100GB × ¥3 = ¥300/月
- 有分层：(5×3 + 15×1 + 30×0.3 + 50×0.12) = ¥51/月
- **节省：83%**

---

## DataAgent 运行方式

### 作为独立器官运行

```python
# P4 数据管理层
class P4DataOrgan:
    """P4 数据管理器官"""
    
    def __init__(self):
        self.generator = DataGenerator()
        self.flow_monitor = DataFlowMonitor()
        self.quality_agent = DataQualityAgent()
        self.archive_manager = DataArchiveManager()
    
    async def life_cycle(self):
        """数据管理生命周期"""
        while True:
            # 持续监控 (每5秒)
            await self.flow_monitor.check_all_meridians()
            
            # 质量检查 (每小时)
            if self._is_hour_start():
                await self.quality_agent.full_check()
            
            # 生命周期转换 (每天凌晨2点)
            if self._is_2am():
                await self.archive_manager.lifecycle_transition()
            
            await asyncio.sleep(5)
```

### 与P0-P3的关系

```
P0感知 ──┐
P1判断 ──┼──► DataAgent监控 ──► 质量报告
P2关系 ──┤         ↓
P3进化 ──┘    异常时告警/修复
     ↑
DataAgent提供历史数据支持进化
```

---

## 实施建议

### 阶段1：基础监控 (2周)
- 数据生成登记
- 基础流转监控
- 简单健康检查

### 阶段2：质量治理 (2周)
- 质量规则引擎
- 自动修复机制
- 健康度看板

### 阶段3：生命周期 (2周)
- 分层存储
- 自动归档
- 成本优化

### 阶段4：智能优化 (持续)
- 基于P3进化优化策略
- 预测性扩容
- 智能压缩

---

## 与数据库选型的关系

| 数据库 | DataAgent支持 |
|--------|---------------|
| SurrealDB | 中等 (需自定义实现) |
| PostgreSQL | 优秀 (TimescaleDB分区 + pg_partman) |
| 专用组合 | 复杂 (需协调多个系统) |

**推荐：PostgreSQL + TimescaleDB**
- 原生支持时序数据分区
- pg_partman自动分区管理
- 成熟的生命周期管理工具

---

## 下一步

**A.** 立即实施DataAgent基础版 (监控+登记)

**B.** 等待数据库选型确定后一起实施

**C.** 先设计数据Schema和生命周期策略

**D.** 需要更多评估
